
    <!DOCTYPE html>
    <html lang="en">
        <head>
        <title>High Available k3s kubernetes cluster with keepalived, galera and longhorn - Raymii.org</title>
        <style> *, ::before, ::after {background-repeat: no-repeat;-webkit-box-sizing: border-box;box-sizing: border-box;}::before, ::after {text-decoration: inherit;vertical-align: inherit;}html {cursor: default;font-family: system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";line-height: 1.15;-moz-tab-size: 4;-o-tab-size: 4;tab-size: 4;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;word-break: break-word;}body {background-color: white;margin: 0;}h1 {font-size: 2em;margin: 0.67em 0;}hr {height: 0;overflow: visible;}main {display: block;}nav ol, nav ul {list-style: none;}pre {font-family: Roboto Mono, Menlo, Consolas, Ubuntu Monospace, Noto Mono, Oxygen Mono, Liberation Mono, monospace;font-size: 1em;}a {background-color: transparent;}abbr[title] {text-decoration: underline;-webkit-text-decoration: underline dotted;text-decoration: underline dotted;}b, strong {font-weight: bolder;}code, kbd, samp {font-family: Menlo, Consolas, Roboto Mono, Ubuntu Monospace, Noto Mono, Oxygen Mono, Liberation Mono, monospace;font-size: 1em;}small {font-size: 80%;}::-moz-selection {background-color: #b3d4fc;color: #000;text-shadow: none;}::selection {background-color: #b3d4fc;color: #000;text-shadow: none;}audio, canvas, iframe, img, svg, video {vertical-align: middle;}audio, video {display: inline-block;}audio:not([controls]) {display: none;height: 0;}img {border-style: none;}svg:not([fill]) {fill: currentColor;}svg:not(:root) {overflow: hidden;}table {border-collapse: collapse;}button, input, select, textarea {font-family: inherit;font-size: inherit;line-height: inherit;}button, input, select {margin: 0;}button {overflow: visible;text-transform: none;}button, [type="button"], [type="reset"], [type="submit"] {-webkit-appearance: button;}fieldset {padding: 0.35em 0.75em 0.625em;}input {overflow: visible;}legend {color: inherit;display: table;max-width: 100%;white-space: normal;}progress {display: inline-block;vertical-align: baseline;}select {text-transform: none;}textarea {margin: 0;overflow: auto;resize: vertical;}[type="checkbox"], [type="radio"] {padding: 0;}[type="search"] {-webkit-appearance: textfield;outline-offset: -2px;}::-webkit-inner-spin-button, ::-webkit-outer-spin-button {height: auto;}::-webkit-input-placeholder {color: inherit;opacity: 0.54;}::-webkit-search-decoration {-webkit-appearance: none;}::-webkit-file-upload-button {-webkit-appearance: button;font: inherit;}::-moz-focus-inner {border-style: none;padding: 0;}:-moz-focusring {outline: 1px dotted ButtonText;}details {display: block;}dialog {background-color: white;border: solid;color: black;display: block;height: -moz-fit-content;height: -webkit-fit-content;height: fit-content;left: 0;margin: auto;padding: 1em;position: absolute;right: 0;width: -moz-fit-content;width: -webkit-fit-content;width: fit-content;}dialog:not([open]) {display: none;}summary {display: list-item;}canvas {display: inline-block;}template {display: none;}a, area, button, input, label, select, summary, textarea, [tabindex] {-ms-touch-action: manipulation;touch-action: manipulation;}[hidden] {display: none;}[aria-busy="true"] {cursor: progress;}[aria-controls] {cursor: pointer;}[aria-disabled="true"], [disabled] {cursor: not-allowed;}[aria-hidden="false"][hidden]:not(:focus) {clip: rect(0, 0, 0, 0);display: inherit;position: absolute;}main, header, footer, article, section, aside, details, summary {margin: 0 auto;margin-bottom: 16px;width: 100%;}main {display: block;margin: 0 auto;max-width: 1000px;padding: 0 16px 16px;}footer {border-top: 1px solid rgba(0, 0, 0, 0.12);padding: 16px 0;text-align: left;}footer p {margin-bottom: 0;}hr {border: 0;border-top: 1px solid rgba(0, 0, 0, 0.12);display: block;margin-top: 16px;margin-bottom: 16px;width: 100%;-webkit-box-sizing: content-box;box-sizing: content-box;height: 0;overflow: visible;}img {height: auto;max-width: 100%;vertical-align: baseline;}@media screen and (max-width: 400px) {article, section, aside {clear: both;display: block;max-width: 100%;}img {margin-right: 16px;}}embed, iframe, video {border: 0;}body {color: rgba(0, 0, 0, 0.8);font-family: "Ubuntu", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 16px;line-height: 1.5;}p {margin: 0;margin-bottom: 16px;}h1, h2, h3, h4, h5, h6 {color: inherit;font-family: inherit;line-height: 1.2;font-weight: 500;}h1 {font-size: 40px;margin: 20px 0 16px;}h2 {font-size: 32px;margin: 20px 0 16px;}h3 {color: #75cc00;font-size: 28px;margin: 16px 0 4px;}h4 {color: #75cc00;font-size: 24px;margin: 16px 0 4px;}h5 {color: #75cc00;font-size: 20px;margin: 16px 0 4px;}h6 {color: #75cc00;font-size: 16px;margin: 16px 0 4px;}small {color: rgba(0, 0, 0, 0.54);vertical-align: bottom;}pre {background: #f7f7f9;color: rgba(0, 0, 0, 0.8);display: block;font-family: "Roboto Mono", Menlo, Monaco, Consolas, "Courier New", monospace;font-size: 16px;margin: 16px 0;padding: 16px;white-space: pre-wrap;overflow-wrap: break-word;}code {background: #f7f7f9;color: rgba(0, 0, 0, 0.8);font-family: "Roboto Mono", Menlo, Monaco, Consolas, "Courier New", monospace;font-size: 16px;line-height: inherit;margin: 0;vertical-align: baseline;word-break: break-all;word-wrap: break-word;}a {color: #75cc00;text-decoration: none;background-color: transparent;}a:hover, a:focus {color: #0062cc;font-weight: bolder;text-decoration: underline;}dl {margin-bottom: 16px;}dd {margin-left: 40px;}ul, ol {margin-bottom: 8px;padding-left: 40px;vertical-align: baseline;}blockquote {border-left: 2px solid rgba(0, 0, 0, 0.8);font-family: Georgia, Times, "Times New Roman", serif;font-style: italic;margin: 16px 0;padding-left: 16px;}figcaption {font-family: Georgia, Times, "Times New Roman", serif;}u {text-decoration: underline;}s {text-decoration: line-through;}sup {font-size: 14px;vertical-align: super;}sub {font-size: 14px;vertical-align: sub;}mark {background: #ffeb3b;}input[type="text"], input[type="password"], input[type="email"], input[type="url"], input[type="date"], input[type="month"], input[type="time"], input[type="datetime"], input[type="datetime-local"], input[type="week"], input[type="number"], input[type="search"], input[type="tel"], select, textarea {background: #fff;background-clip: padding-box;border: 1px solid rgba(0, 0, 0, 0.12);border-radius: 4px;color: rgba(0, 0, 0, 0.8);display: block;width: 100%;padding: 8px 16px;line-height: 1.5;-webkit-transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";}input[type="color"] {background: #fff;border: 1px solid rgba(0, 0, 0, 0.12);border-radius: 4px;display: inline-block;vertical-align: middle;}input:not([type]) {-webkit-appearance: none;background: #fff;background-clip: padding-box;border: 1px solid rgba(0, 0, 0, 0.12);border-radius: 4px;color: rgba(0, 0, 0, 0.8);display: block;width: 100%;padding: 8px 16px;line-height: 1.5;-webkit-transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;text-align: left;}input[type="text"]:focus, input[type="password"]:focus, input[type="email"]:focus, input[type="url"]:focus, input[type="date"]:focus, input[type="month"]:focus, input[type="time"]:focus, input[type="datetime"]:focus, input[type="datetime-local"]:focus, input[type="week"]:focus, input[type="number"]:focus, input[type="search"]:focus, input[type="tel"]:focus, input[type="color"]:focus, select:focus, textarea:focus {background-color: #fff;border-color: #80bdff;outline: 0;-webkit-box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);}input:not([type]):focus {background-color: #fff;border-color: #80bdff;outline: 0;-webkit-box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);}input[type="file"]:focus, input[type="radio"]:focus, input[type="checkbox"]:focus {outline: 1px thin rgba(0, 0, 0, 0.12);}input[type="text"][disabled], input[type="password"][disabled], input[type="email"][disabled], input[type="url"][disabled], input[type="date"][disabled], input[type="month"][disabled], input[type="time"][disabled], input[type="datetime"][disabled], input[type="datetime-local"][disabled], input[type="week"][disabled], input[type="number"][disabled], input[type="search"][disabled], input[type="tel"][disabled], input[type="color"][disabled], select[disabled], textarea[disabled] {background-color: rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.54);cursor: not-allowed;opacity: 1;}input:not([type])[disabled] {background-color: rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.54);cursor: not-allowed;opacity: 1;}input[readonly], select[readonly], textarea[readonly] {border-color: rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.54);}input:focus:invalid, textarea:focus:invalid, select:focus:invalid {border-color: #ea1c0d;color: #f44336;}input[type="file"]:focus:invalid:focus, input[type="radio"]:focus:invalid:focus, input[type="checkbox"]:focus:invalid:focus {outline-color: #f44336;}select {border: 1px solid rgba(0, 0, 0, 0.12);vertical-align: sub;}select:not([size]):not([multiple]) {height: -webkit-calc(2.25rem + 2px);height: calc(2.25rem + 2px);}select[multiple] {height: auto;}label {display: inline-block;line-height: 2;}fieldset {border: 0;margin: 0;padding: 8px 0;}legend {border-bottom: 1px solid rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.8);display: block;margin-bottom: 8px;padding: 8px 0;width: 100%;}textarea {overflow: auto;resize: vertical;}input[type=checkbox], input[type=radio] {-webkit-box-sizing: border-box;box-sizing: border-box;padding: 0;display: inline;}input[type=submit], input[type=reset], input[type=button], button {background-color: #75cc00;border: #75cc00;border-radius: 4px;color: #fff;padding: 8px 16px;display: inline-block;font-weight: 400;text-align: center;white-space: nowrap;vertical-align: middle;-webkit-user-select: none;-moz-user-select: none;-ms-user-select: none;user-select: none;border: 1px solid transparent;font-size: 1rem;line-height: 1.5;-webkit-transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, box-shadow .15s ease-in-out;transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, box-shadow .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;}input[type=submit]::-moz-focus-inner, input[type=reset]::-moz-focus-inner, input[type=button]::-moz-focus-inner, button::-moz-focus-inner {padding: 0;}input[type=submit]:hover, input[type=reset]:hover, input[type=button]:hover, button:hover {background-color: #0069d9;border-color: #0062cc;color: #fff;}input[type=submit]:not(:disabled):active, input[type=reset]:not(:disabled):active, input[type=button]:not(:disabled):active, button:not(:disabled):active {background-color: #0062cc;border-color: #005cbf;color: #fff;}input[type=submit]:focus, input[type=reset]:focus, input[type=button]:focus, button:focus {outline: 0;-webkit-box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.5);box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.5);}input[type=submit]:disabled, input[type=reset]:disabled, input[type=button]:disabled, button:disabled {opacity: .65;cursor: not-allowed;background-color: #75cc00;border-color: #75cc00;color: #fff;}table {border-top: 1px solid rgba(0, 0, 0, 0.12);margin-bottom: 16px;}caption {padding: 8px 0;}thead th {border: 0;border-bottom: 2px solid rgba(0, 0, 0, 0.12);text-align: left;}tr {margin-bottom: 8px;}th, td {border-bottom: 1px solid rgba(0, 0, 0, 0.12);padding: 16px;white-space: nowrap;vertical-align: inherit;}tfoot tr {text-align: left;}tfoot td {color: rgba(0, 0, 0, 0.54);font-size: 8px;font-style: italic;padding: 16px 4px;}a.skip-main {left:-999px;position:absolute;top:auto;width:1px;height:1px;overflow:hidden;z-index:-999;}a.skip-main:focus, a.skip-main:active {color: #fff;background-color:#000;left: auto;top: auto;width: 30%;height: auto;overflow:auto;margin: 10px 35%;padding:5px;border-radius: 15px;border:4px solid yellow;text-align:center;font-size:1.2em;z-index:999;}@font-face {font-family: 'Raleway';font-style: normal;font-weight: 600;src: url('/s/inc/css/raleway-v18-latin-600.eot');src: local(''), url('/s/inc/css/raleway-v18-latin-600.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/raleway-v18-latin-600.woff2') format('woff2'), url('/s/inc/css/raleway-v18-latin-600.woff') format('woff'), url('/s/inc/css/raleway-v18-latin-600.ttf') format('truetype'), url('/s/inc/css/raleway-v18-latin-600.svg#Raleway') format('svg');}@font-face {font-family: 'Raleway';font-style: italic;font-weight: 400;src: url('/s/inc/css/raleway-v18-latin-italic.eot');src: local(''), url('/s/inc/css/raleway-v18-latin-italic.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/raleway-v18-latin-italic.woff2') format('woff2'), url('/s/inc/css/raleway-v18-latin-italic.woff') format('woff'), url('/s/inc/css/raleway-v18-latin-italic.ttf') format('truetype'), url('/s/inc/css/raleway-v18-latin-italic.svg#Raleway') format('svg');}@font-face {font-family: 'Roboto Mono';font-style: normal;font-weight: 400;src: url('/s/inc/css/roboto-mono-v12-latin-regular.eot');src: local(''), url('/s/inc/css/roboto-mono-v12-latin-regular.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/roboto-mono-v12-latin-regular.woff2') format('woff2'), url('/s/inc/css/roboto-mono-v12-latin-regular.woff') format('woff'), url('/s/inc/css/roboto-mono-v12-latin-regular.ttf') format('truetype'), url('/s/inc/css/roboto-mono-v12-latin-regular.svg#RobotoMono') format('svg');}@font-face {font-family: 'Roboto Mono';font-style: normal;font-weight: 600;src: url('/s/inc/css/roboto-mono-v12-latin-600.eot');src: local(''), url('/s/inc/css/roboto-mono-v12-latin-600.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/roboto-mono-v12-latin-600.woff2') format('woff2'), url('/s/inc/css/roboto-mono-v12-latin-600.woff') format('woff'), url('/s/inc/css/roboto-mono-v12-latin-600.ttf') format('truetype'), url('/s/inc/css/roboto-mono-v12-latin-600.svg#RobotoMono') format('svg');}@font-face {font-family: 'Ubuntu';font-style: normal;font-weight: 400;src: url('/s/inc/css/ubuntu-v15-latin-regular.eot');src: local(''), url('/s/inc/css/ubuntu-v15-latin-regular.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/ubuntu-v15-latin-regular.woff2') format('woff2'), url('/s/inc/css/ubuntu-v15-latin-regular.woff') format('woff'), url('/s/inc/css/ubuntu-v15-latin-regular.ttf') format('truetype'), url('/s/inc/css/ubuntu-v15-latin-regular.svg#Ubuntu') format('svg');}@font-face {font-family:'Raleway2';font-style:normal;font-weight:normal;src:url('/s/inc/css/raleway.eot');src:local('Raleway2'),local('Raleway2'),url('/s/inc/css/raleway.ttf') }.headheader {font-family:"Raleway2"!important }.headheader a {color:#000;text-decoration:none }.headheader a:hover {color:#000;text-decoration:none!important }#toc ul {list-style: none;margin: 0;padding: 0;}#toc h3 {color:black;}</style>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link type="application/opensearchdescription+xml" rel="search" href="/s/inc/opensearch.xml"/>
        <link rel="alternate" type="application/rss+xml" title="RSS Feed for Raymii.org" href="https://raymii.org/s/feed.xml" />         
    </head>
    <body>
        
        <a id="top-of-page"></a>
        <main>
        <a class="skip-main" href="#main">Skip to main content</a>
            <header>
                <h1 class="headheader">
                    <a href="https://raymii.org/s/">Raymii.org 
                        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAKCAYAAAD2Fg1xAAABgElEQVQ4jb3VP0iVYRTH8c9waXBokog7OYhTXChuF3GIi4hoiJA4REQIOTgGtoWTg6ODs0SYComIXCJEMhpKtD9guUU0ujRFS0PQ8DzC24v3Pq+3S9/pnMOP8/7Ocx6el/OziRN0JXTD+I2xhK4WdeNteGmbu8IgC3jQQlfCZ0zgINHzJabwoQP+ClHGV1zGJXwRDJ/FDJZi3MBQE10dL2K8gZFOGE3REDZyyjLunKG7KAzZHfMaXjXp+QbXYlzBfrvmSuhBNaHrxQU8zdQW8RhrOe0snuB7zA/jd6p4n9HV8QMfY/4JPzGAt7meFfS18LdXEk7uemIQuJ/Lj6PZQezFWhm3cTWnXcAj3MrU5oWh5WpzGM3UurGNZy28HSa8J7mB3Uy+4u/rl+UdrsT4Jraa6F6jP5M3MP0PHguzL9zzqmC2GRNYjXF2qDzDwgbgHp53wGMhJrEunGQ9oT3CQ+GFasWBsLVvwiv5XygJz/JOAe208POrJHST+CVspBB/AFY9Q3+QJqLxAAAAAElFTkSuQmCC" alt="Raymii.org Logo">
                    </a>
                </h1>
                <small>
                  Quis custodiet ipsos custodes?<br>
                  <a href="/s/">Home</a> | 
                  <a href="/s/static/About.html">About</a> | 
                  <a href="/s/tags/all.html">All pages</a> | 
                  <a href="/s/software/Sparkling_Network.html">Cluster Status</a> | 
                  <a href="https://raymii.org/s/feed.xml">RSS Feed</a> 
                </small><br/><p>
                <link href="/s/_pagefind/pagefind-ui.css" rel="stylesheet">
                <script src="/s/_pagefind/pagefind-ui.js" type="text/javascript"></script>
                <div id="search" style="min-width:400px;max-width:1080px;"></div>
                <script>
                    window.addEventListener('DOMContentLoaded', (event) => {
                        new PagefindUI({ element: "#search" });
                    });
                </script>
                </p>
            </header>
          
    <main data-pagefind-body><h2 class='headheader' data-pagefind-meta='title' id='main'>High Available k3s kubernetes cluster with keepalived, galera and longhorn</h2>
<p><small>Published: <span data-pagefind-meta='date'>09-07-2024 22:30</span> | Author: Remy van Elst | <a href="High_Available_k3s_kubernetes_cluster_with_keepalived_galera_and_longhorn.txt">Text only version of this article</a>
</small></p>
<br><div id="toc">
<h3>Table of Contents</h3>
<ul>
<li>
<a href="#toc_0">Overview</a>
</li>
<li>
<a href="#toc_1">keepalived</a>
</li>
<li>
<a href="#toc_2">MariaDB Galera cluster</a>
<ul>
<li>
<a href="#toc_3">Galera Cluster auto-reboot</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">k3s mysql database setup</a>
</li>
<li>
<a href="#toc_5">Installing K3S</a>
</li>
<li>
<a href="#toc_6">HA storage in k3s</a>
</li>
<li>
<a href="#toc_7">Testing</a>
</li>
</ul>

</div><hr><div id="contents">
<p>After my <a href="/s/tutorials/My_First_Kubernetes_k3s_cluster_on_3_Orange_Pi_Zero_3s_including_k8s_dashboard_hello-node_and_failover.html">first adventure with Kubernetes</a>, getting started with k3s on my small 3 node ARM cluster that <a href="/s/tutorials/Netboot_PXE_Armbian_on_an_Orange_Pi_Zero_3_from_SPI_with_NFS_root_filesystem.html">boots via PXE / NFS</a>, I noticed that there is only one k3s node that has the <code>control-plane,master</code> role. If that node fails you can no longer manager the cluster. Other nodes can fail and then the workloads (pods) will be restarted eventually after 5 minutes, but this node is special. Time to change that and make it a high available cluster.
K3s <a href="https://web.archive.org/web/20240703112841/https://docs.k3s.io/datastore/ha">supports</a> high-availability with embedded <code>etcd</code> and with external databases like <code>MySQL</code> and <code>postgres</code>. <code>etcd</code> will thrash your storage (SD cards) so I decided to go with a <code>MySQL</code> cluster using <code>Galera</code> for the database and <code>keepalived</code> for the High Available Cluster IP. This guide will show you how to configure the HA database and HA-IP and I&#39;ll also setup <a href="https://web.archive.org/web/20240707025724/https://longhorn.io/">longhorn</a> for high-available block storage inside kubernetes. The end result is that I can pull the power from any two of the three nodes without the k3s cluster or workloads going down. </p>

<p class="ad"> <b>Recently I removed all Google Ads from this site due to their invasive tracking, as well as Google Analytics. Please, if you found this content useful, consider a small donation using any of the options below:</b><br><br> <a href="https://leafnode.nl">I'm developing an open source monitoring app called  Leaf Node Monitoring, for windows, linux & android. Go check it out!</a><br><br> <a href="https://github.com/sponsors/RaymiiOrg/">Consider sponsoring me on Github. It means the world to me if you show your appreciation and you'll help pay the server costs.</a><br><br> <a href="https://www.digitalocean.com/?refcode=7435ae6b8212">You can also sponsor me by getting a Digital Ocean VPS. With this referral link you'll get $200 credit for 60 days. Spend $25 after your credit expires and I'll get $25!</a><br><br> </p>

<p><strong>You need at least 3 nodes for a cluster!</strong>. 2 nodes are prone to split-brain
  situations and harder to setup. If you go beyond 3 nodes, make sure you
  always have an odd-number of cluster components (3, 5, 7, etc) so in the
  case of a failure there is always a majority for the quorum.</p>

<p>My k3s cluster now has 4 nodes, but one of them is only a worker node, not
part of the high-available control plane setup, so technically, the
high-available cluster uses 3 nodes.</p>

<h3 id="toc_0">Overview</h3>

<p>Here is a high-level diagram of the setup:</p>

<p><img src="/s/inc/img/ha-k3s-1.png" alt="ha-k3s.png"></p>

<p>The blue parts (<code>keepalived</code> &amp; <code>galera</code>) on the diagram inside a node provide
the high-availability for the kubernetes (k3s) control plane.  For other
high-available options for k3s, consult <a href="https://web.archive.org/web/20240703112841/https://docs.k3s.io/datastore/ha">their documentation</a>.</p>

<p><code>keepalived</code> provides a high-available IP via VRRP. All three nodes run
<code>keepalived</code>, one of them being the <code>MASTER</code>, the other two are the <code>BACKUP</code>
nodes. This HA-IP is what the <code>Galera</code> database will use and where all k3s
nodes (that are not part of the HA control plane) will communicate with. I&#39;m
also using it as a <code>LoadBalancer</code> to make services accessible in the cluster
from the outside. </p>

<p><code>Galera</code> is <a href="https://galeracluster.com/products/">a synchronous multi-master replication plug-in</a> for InnoDB. A client can read and write
to any node in a cluster and changes are applied to all servers.
Galera-cluster is <a href="https://web.archive.org/web/20240707034001/https://mariadb.com/kb/en/what-is-mariadb-galera-cluster/">built in to MariaDB</a>
nowadays. I used to use Percona ExtraDB but since it&#39;s now built in to
MariaDB I decided to use that. Galera makes MySQL clustering very easy. Next
to the multi-master part (no failover since there are no slaves) it also
automatically syncs up any new nodes, skipping lengthy manual provisioning.</p>

<p>The rest of the diagram inside of a node runs &quot;inside&quot; of kubernetes. </p>

<p><a href="https://web.archive.org/web/20240707025724/https://longhorn.io/">Longhorn</a> provides
high-available block storage. Any <code>Persistent Volumes</code> in you kubernetes
cluster are replicated over all three nodes including automated failover. In
k3s, by default, volumes use local storage, meaning that if a node goes down,
the volume is not accessible on the other node where the pods will start up
again. With <code>Longhorn</code> that problem is gone, making the k3s cluster truly
high-available. Otherwise our control-plane would survive a node failure, but
the workloads would not function correctly afterwards.</p>

<p>The rest of the article will focus on setting up the individual components.
For simplicity&#39;s sake I&#39;m showing the manual setup. The parts are small
enough to easily convert to Ansible or your favorite configuration management
tool.</p>

<p>It&#39;s important that you install the components in order shown here. First
<code>keepalived</code>, then <code>galera</code>, then <code>k3s</code>. Before deploying any workload inside
of kubernetes, make sure you have setup <code>Longhorn</code> for HA-storage.</p>

<p>My cluster uses <a href="https://www.armbian.com/">Armbian</a>, a Debian distribution for
small board computers. The guide should work on regular debian as well.</p>

<p>The High-Available IP address (managed by <code>keepalived</code>) for the cluster is:</p>

<ul>
<li><code>192.0.2.50</code></li>
</ul>

<p>The 3 cluster nodes have the following IP addresses:</p>

<ul>
<li><code>192.0.2.60</code></li>
<li><code>192.0.2.61</code></li>
<li><code>192.0.2.62</code></li>
</ul>

<p>The worker node in the cluster has the following IP address:</p>

<ul>
<li><code>192.0.2.63</code></li>
</ul>

<p>The HA-IP (<code>192.0.2.50</code>) must be set up before the database or k3s can be
installed. </p>

<h3 id="toc_1">keepalived</h3>

<p>I&#39;ve been using <a href="/s/tutorials/Keepalived-Simple-IP-failover-on-Ubuntu.html">keepalived for over 10 years</a>, 
that article is from 2014. For a simple setup with just an HA-IP like 
this I prefer it over <code>corosync</code> since it&#39;s easier to setup.</p>

<p>Install the required components on all 3 nodes:</p>

<pre><code>apt install dirmngr software-properties-common rsync keepalived
</code></pre>

<p>Choose one of your 3 nodes to be the master node. This node will by default
have the high-available IP. It does not matter which node it is, although if
your cluster has different hardware it&#39;s best to choose the most powerful
node.</p>

<p>Edit the configuration file:</p>

<pre><code>vim /etc/keepalived/keepalived.conf
</code></pre>

<p>Place the following:  </p>

<pre><code>vrrp_instance VI_1 {
  state MASTER
  interface end0
  virtual_router_id 51
  priority 100
  advert_int 1
  authentication {
    auth_type PASS
    auth_pass 
  }
  virtual_ipaddress {
    192.0.2.50
  }
}
</code></pre>

<p>Change the following parameters:</p>

<ul>
<li><code>interface</code>: your physical network interface, use <code>ip addr</code> to find it</li>
<li><code>auth_pass</code> to a secret password.</li>
<li><code>virtual_ipaddress</code> to the HA-IP you want to use.</li>
</ul>

<p>On the other two nodes, also edit the configuration file:</p>

<pre><code>vim /etc/keepalived/keepalived.conf
</code></pre>

<p>Place the following:</p>

<pre><code>vrrp_instance VI_1 {
  state BACKUP
  interface end0
  virtual_router_id 51
  priority 
  advert_int 1
  authentication {
    auth_type PASS
    auth_pass 
  }
  virtual_ipaddress {
    192.0.2.50
  }
}
</code></pre>

<p>Change the following parameters:</p>

<ul>
<li><code>auth_pass</code> to match the password in the master node config file</li>
<li><code>priority</code>: each node must have a different (lower) priority. Node2 should be <code>50</code>, Node3 should be <code>49</code> and so on. </li>
<li><code>interface</code>: your physical network interface, use <code>ip addr</code> to find it</li>
<li><code>virtual_ipaddress</code> to the HA-IP you configured on the master node.</li>
</ul>

<p>After configuring <code>keepalived</code> restart the service on all three nodes:</p>

<pre><code>systemctl restart keepalived
</code></pre>

<p>You can test the HA-IP by logging in via <code>ssh</code> to the HA-IP for example. Or you can send
<code>SIGUSR1</code> to the <code>keepalived</code> process and check <code>/tmp/keepalived.data</code>:</p>

<pre><code>kill -SIGUSR1 $(pidof keepalived)
cat  /tmp/keepalived.data
</code></pre>

<p>Output:   </p>

<pre><code>------&lt; Global definitions &gt;------
[...]
 VRRP IPv4 mcast group = 224.0.0.18
 VRRP IPv6 mcast group = ff02::12
 Gratuitous ARP delay = 5
 Gratuitous ARP repeat = 5
 [...]
------&lt; VRRP Topology &gt;------
 VRRP Instance = VI_1
   VRRP Version = 2
   State = BACKUP
   Master router = 192.0.2.60
   Master priority = 100
   Flags: none
   Wantstate = BACKUP
   Number of config faults = 0
   Number of interface and track script faults = 0
   Number of track scripts init = 0
   Last transition = 1720124853.373151 (Thu Jul  4 20:27:33.373151 2024)
   Read timeout = 1720131680.650708 (Thu Jul  4 22:21:20.650708 2024)
   Master down timer = 3804687 usecs
   Interface = end0
   Using src_ip = 192.0.2.61
   Multicast address 224.0.0.18
   [...]
   Virtual Router ID = 51
   Priority = 50
   Effective priority = 50
   Total priority = 50
   Advert interval = 1 sec
   Virtual IP (1):
     192.0.2.50 dev end0 scope global
   fd_in 13, fd_out 14
  [...]
</code></pre>

<p>Test the <code>keepalived</code> failover by shutting down your master node. Check the
above file on the two other nodes and login via SSH on the HA-IP to see that
the other node has taken over. </p>

<h3 id="toc_2">MariaDB Galera cluster</h3>

<p>On all three nodes, add the <a href="https://mariadb.com/kb/en/installing-mariadb-deb-files/">MariaDB APT repository</a>:</p>

<pre><code>add-apt-repository &#39;deb http://mirrors.digitalocean.com/mariadb/repo/11.5/debian bookworm main&#39;
apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8
</code></pre>

<p>On their page there is also a setup script, but that requires <a href="https://web.archive.org/web/20240707041822/https://sysdig.com/blog/friends-dont-let-friends-curl-bash/">piping curl to
bash</a>
which is very insecure. Setting up the repository manually is not much more
difficult.</p>

<p>Update the local cache and install mariadb:</p>

<pre><code>apt update
apt install mariadb-server 
</code></pre>

<p><strong>This article exposes MariaDB to your network. On a real production setup,
  please use separate VLAN&#39;s for management and this kind of traffic</strong>. For a
  home-lab setup like mine, this is fine.</p>

<p>Make sure <code>mariadb</code> is accessible over the network (instead of localhost):</p>

<pre><code>vim  /etc/mysql/mariadb.conf.d/50-server.cnf
</code></pre>

<p>Update <code>bind-address</code>, change <code>127.0.0.1</code> to <code>0.0.0.0</code>:</p>

<pre><code># Instead of skip-networking the default is now to listen only on
# localhost which is more compatible and is not less secure.
bind-address            = 0.0.0.0
</code></pre>

<p>Make sure the <code>root</code> user has a secure password. By default that user has no password. Open a MySQL prompt:</p>

<pre><code>mariadb -uroot
</code></pre>

<p>Set a secure password:</p>

<pre><code>set password = password(&quot;SuperS3cUr3P@ssw0rd&quot;);
quit;
</code></pre>

<p>Now it&#39;s time to set up the Galera Cluster. Create the configuration file:</p>

<pre><code>vim /etc/mysql/conf.d/galera.cnf
</code></pre>

<p>Paste the following:</p>

<pre><code>[mysqld]
binlog_format=ROW
default-storage-engine=innodb
innodb_autoinc_lock_mode=2
bind-address=0.0.0.0

# Galera Provider Configuration
wsrep_on=ON
wsrep_provider=/usr/lib/galera/libgalera_smm.so

# Galera Cluster Configuration
wsrep_cluster_name=&quot;k3s_cluster&quot;
wsrep_cluster_address=&quot;gcomm://192.0.2.60,192.0.2.61,192.0.2.62&quot;

# Galera Synchronization Configuration
wsrep_sst_method=rsync

# Galera Node Configuration
wsrep_node_address=&quot;192.0.2.60&quot;
wsrep_node_name=&quot;node1&quot;
</code></pre>

<p>Change the following parameters to be the same on all 3 nodes:</p>

<ul>
<li><code>wsrep_cluster_name</code>: the name of your cluster </li>
<li><code>wsrep_cluster_address</code>: add all IP&#39;s of the cluster nodes</li>
</ul>

<p>Change the following parameters on each node, unique for that specific node:</p>

<ul>
<li><code>wsrep_node_address</code>: IP address of that node</li>
<li><code>wsrep_node_name</code> name for that node</li>
</ul>

<p>Time to bootstrap the cluster. Stop MySQL on all nodes:</p>

<pre><code>systemctl stop mysql  
</code></pre>

<p>On node 1, start up galera:</p>

<pre><code>galera_new_cluster # no output
</code></pre>

<p>Check:</p>

<pre><code>mysql -u root -p -e &quot;SHOW STATUS LIKE &#39;wsrep_cluster_size&#39;&quot;
</code></pre>

<p>Output:</p>

<pre><code>+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 1     |
+--------------------+-------+
</code></pre>

<p>On node 2 (and node3):</p>

<pre><code>systemctl start mariadb
</code></pre>

<p>Check cluster status on node 1:</p>

<pre><code>+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 2     |
+--------------------+-------+
</code></pre>

<p>Repeat the above step on <code>node3</code>, the value should increase to 3.</p>

<h4 id="toc_3">Galera Cluster auto-reboot</h4>

<p>Normally you would not want to do this and assume the cluster is up. In my
case I turn off the Orange Pi Zero 3 computers when I&#39;m experimenting for the
day. When <a href="https://web.archive.org/web/20240709040142/https://galeracluster.com/library/training/tutorials/restarting-cluster.html">restarting a Galera Cluster</a>
you need to follow specific steps and do checks to make sure everything
starts up correctly without data loss. The same goes for when one node drops
out. </p>

<p>But that also means that every start of my cluster I manually need to
intervene and <code>fix</code> the Galera cluster. In this case I don&#39;t care much about
data loss or correct startup order, I want things to boot without manual
intervention. So here is something <strong>you should never do in production</strong>.</p>

<p>Edit the <code>mariadb</code> systemd service:</p>

<pre><code>systemctl edit mariadb  
</code></pre>

<p>Add the following (in between the comment lines):</p>

<pre><code>### Editing /etc/systemd/system/mariadb.service.d/override.conf
### Anything between here and the comment below will become the new contents of the file

[Service]
Restart=on-failure
RestartSec=5s

### Lines below this comment will be discarded
</code></pre>

<p>By default <code>mariadb</code> only restarts <code>on-abort</code>, not <code>on-failure</code>. I want it to
retry starting up continuesly.</p>

<p>Next is the <strong>most dangerous part</strong>, a cronjob to forcefully &quot;fix&quot; the  galera
cluster:</p>

<pre><code>* *  *   *   *   sleep 30 ; /usr/bin/systemctl is-active --quiet mariadb.service || { /usr/bin/sed -i &#39;s/safe_to_bootstrap: 0/safe_to_bootstrap: 1/g&#39; /var/lib/mysql/grastate.dat ; /usr/bin/galera_new_cluster || { sleep 60 ; /sbin/reboot; } } 2&gt;&amp;1 | /usr/bin/logger
</code></pre>

<p>This cronjob will wait 30 seconds and then, if the <code>mariadb</code> service is not
running, set <code>safe_to_bootstrap</code> to <code>1</code>. Then it <code>bootstraps</code> the cluster. If
that bootstrapping fails, it sleeps for a minute and reboots. </p>

<p>If <code>mariadb</code> is running it does nothing. If the bootstrapping succeeds, it
also does not reboot.</p>

<p>In normal operations I issue the shutdown command like this:</p>

<pre><code>ssh -o ConnectTimeout=5 -o BatchMode=yes -o StrictHostKeyChecking=no root@192.0.2.60 poweroff; 
ssh -o ConnectTimeout=5 -o BatchMode=yes -o StrictHostKeyChecking=no root@192.0.2.61 poweroff;
sleep 30;
ssh -o ConnectTimeout=5 -o BatchMode=yes -o StrictHostKeyChecking=no root@192.0.2.62 poweroff; 
</code></pre>

<p>With this wait time it results in an &quot;orderly&quot; shutdown with one node being
the &quot;last&quot; node for the Galera cluster, so most of the time this cronjob does
nothing. </p>

<p>Continuing on with the k3s parts of the database setup.</p>

<h3 id="toc_4">k3s mysql database setup</h3>

<p>This is simple and straightforward, just like you would normally create a
database and user. Login to the database:</p>

<pre><code>mysql -uroot
</code></pre>

<p>Create the specific database for <code>k3s</code>:</p>

<pre><code>CREATE DATABASE k3s;
</code></pre>

<p>Create a user with password for <code>k3s</code>:</p>

<pre><code>CREATE USER &#39;k3s&#39;@&#39;%&#39; IDENTIFIED BY &#39;Supers3cr3tP4ssw0rd&#39;;
</code></pre>

<p>Grant that user all permissions on that database:</p>

<pre><code>GRANT ALL PRIVILEGES ON k3s.* TO &#39;k3s&#39;@&#39;%&#39;;
FLUSH PRIVILEGES;
</code></pre>

<h3 id="toc_5">Installing K3S</h3>

<p>On the first node (of the three) you install K3S like you would normally do
but with two extra arguments. </p>

<p>The first is <code>--tls-san=&quot;$HA-IP&quot;</code>, the Keepalived high available cluster IP.
All nodes will connect to k3s and the database via this high available IP.</p>

<p>The second argument is <code>--datastore-endpoint</code>, which is where you provide your
newly created database connection information, using the HA-IP as well.</p>

<pre><code>curl -sfL https://get.k3s.io | sh -s - \
  --datastore-endpoint=&quot;mysql://k3s:Supers3cr3tP4ssw0rd@tcp(192.0.2.50:3306)/k3s&quot; \
  --tls-san=&quot;192.0.2.50&quot;
</code></pre>

<p>After installation finishes, get the token. You need that on the other nodes:</p>

<pre><code>cat /var/lib/rancher/k3s/server/node-token
</code></pre>

<p>Output:</p>

<pre><code>K10a[...]418::server:7a8[...]8e441
</code></pre>

<p>On the second and third master node, use the following command. It&#39;s the same
as above only the token is provided extra:</p>

<pre><code>curl -sfL https://get.k3s.io | sh -s - server \
  --datastore-endpoint=&quot;mysql://k3s:Supers3cr3tP4ssw0rd@tcp(192.0.2.50:3306)/k3s&quot; \
  --token=&quot;K10a[...]418::server:7a8[...]8e441&quot; \
  --tls-san=&quot;192.0.2.50&quot;
</code></pre>

<p>Installation of extra agents is done like normal, just the token and the HA-IP:</p>

<pre><code>curl -sfL https://get.k3s.io | \
  K3S_URL=&quot;https://192.0.2.50:6443&quot; \
  K3S_TOKEN=&quot;K10a[...]418::server:7a8[...]8e441&quot; \
  sh -s -
</code></pre>

<p>After installation you can issue the following command on any of the master nodes:</p>

<pre><code>kubectl get nodes
</code></pre>

<p>Output:</p>

<pre><code>NAME            STATUS   ROLES                  AGE     VERSION
opz3-1-onder    Ready    control-plane,master   7m23s   v1.29.6+k3s1
opz3-2-midden   Ready    control-plane,master   9m50s   v1.29.6+k3s1
opz3-3-boven    Ready    control-plane,master   7m18s   v1.29.6+k3s1
opz3-4-top      Ready    &lt;none&gt;                 14s     v1.29.6+k3s1
</code></pre>

<p>There are 3 master nodes now! </p>

<p>We&#39;re almost done with the high-available part. The only thing to do is
install <code>longhorn</code> in the cluster so that our pod storage(persistent volumes)
is high available as well.</p>

<p>Please <a href="/s/tutorials/My_First_Kubernetes_k3s_cluster_on_3_Orange_Pi_Zero_3s_including_k8s_dashboard_hello-node_and_failover.html#toc_2">consult my other article</a>
on how to set up your local workstation with <code>kubectl</code> and <code>helm</code>. You will
need that for the rest of this article.</p>

<h3 id="toc_6">HA storage in k3s</h3>

<p><img src="/s/inc/img/ha-k3s-2.png" alt="longhorn.png"></p>

<blockquote>
<p>A screenshot of my longhorn dashboard</p>
</blockquote>

<p><code>longhorn</code> is a distributed block storage system which makes the persistent
volumes in kubernetes available on multiple nodes and keeps those in sync. If
one node fails, pods get started up on another node and storage is still
available. Without <code>longhorn</code>, the persistent volume would not be available
since that, in the case of k3s, is on the local node.</p>

<p><strong>This article exposes iSCSI to your network. On a real production setup,
  please use separate VLAN&#39;s for storage traffic</strong>. For a home-lab setup like
  mine, this is fine.</p>

<p>Start by installing the following package on all <code>k3s</code> nodes:</p>

<pre><code>apt-get install open-iscsi
</code></pre>

<p>On your <a href="/s/tutorials/My_First_Kubernetes_k3s_cluster_on_3_Orange_Pi_Zero_3s_including_k8s_dashboard_hello-node_and_failover.html#toc_2">local admin workstation</a> you must create a folder for the yaml files:</p>

<pre><code>mkdir longhorn
cd longhorn
</code></pre>

<p>Download the deployment file for longhorn:</p>

<pre><code>wget https://raw.githubusercontent.com/longhorn/longhorn/v1.6.0/deploy/longhorn.yaml
</code></pre>

<p>Apply it with  <code>kubectl</code>:</p>

<pre><code>kubectl apply -f longhorn.yaml
</code></pre>

<p>For testing, expose the dashboard on port <code>8877</code>. There is no login required
by default. In a follow up article I&#39;ll show you how to setup an <code>Ingress</code>
with a password, but for now this is fine.</p>

<pre><code>kubectl expose service longhorn-frontend --type=LoadBalancer --port=8877 --target-port 8000  --name=longhorn-frontend-ext --namespace longhorn-system
</code></pre>

<p>You should now be able to navigate in your browser to <code>http://ha-ip:8877</code> and
see the longhorn dashboard.</p>

<p>We need to make the <code>longhorn</code> storage be the default storage. Otherwise,
unchanged helm charts and deployments still would use the <code>local-path</code>
storage (per node). This step is <code>k3s</code> specific.</p>

<p>Check the current storage classes:</p>

<pre><code>kubectl get storageclass -A
</code></pre>

<p>Output:</p>

<pre><code>NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  18m
longhorn (default)     driver.longhorn.io      Delete          Immediate              true                   3m48s  
</code></pre>

<p>Use the following command to remove <code>local-path</code> as default option:</p>

<pre><code>kubectl patch storageclass local-path -p &#39;{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;false&quot;}}}&#39;  storageclass.storage.k8s.io/local-path patched
</code></pre>

<p>Check the storage classes again:</p>

<pre><code>kubectl get storageclass -A
</code></pre>

<p>Output:</p>

<pre><code>NAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path           rancher.io/local-path   Delete          WaitForFirstConsumer   false                  18m
longhorn (default)   driver.longhorn.io      Delete          Immediate              true                   4m23s
</code></pre>

<p>The last thing you need to do is go to the dashboard, settings and search for
<code>Pod Deletion Policy When Node is Down</code>. By default it&#39;s set to <code>Do Nothing</code>.
In my case, I set it to <code>delete-both-statefulset-and-deployment-pod</code>.  </p>

<blockquote>
<p>Longhorn will force delete StatefulSet/Deployment terminating pods on nodes
  that are down to release Longhorn volumes so that Kubernetes can spin up
  replacement pods.</p>
</blockquote>

<p>If you do not change this and a node fails, the other nodes will not be able to start the pods with the following error: </p>

<pre><code>the volume is currently attached to different node (the node that is down)
</code></pre>

<p>The <a href="https://web.archive.org/web/20240709044003/https://longhorn.io/docs/1.6.0/high-availability/node-failure/#what-to-expect-when-a-kubernetes-node-fails">longhorn documentation</a> explains what this setting does:</p>

<blockquote>
<p>This section is aimed to inform users of what happens during a node failure
  and what is expected during the recovery.</p>

<p>After one minute, kubectl get nodes will report NotReady for the failure
  node.</p>

<p>After about five minutes, the states of all the pods on the NotReady node
  will change to either Unknown or NodeLost.</p>

<p>StatefulSets have a stable identity, so Kubernetes won&#39;t force delete the
  pod for the user. See the official Kubernetes documentation about forcing
  the deletion of a StatefulSet.</p>

<p>Deployments don&#39;t have a stable identity, but for the Read-Write-Once type
  of storage, since it cannot be attached to two nodes at the same time, the
  new pod created by Kubernetes won&#39;t be able to start due to the RWO volume
  still attached to the old pod, on the lost node.</p>

<p>In both cases, Kubernetes will automatically evict the pod (set deletion
  timestamp for the pod) on the lost node, then try to recreate a new one
  with old volumes. Because the evicted pod gets stuck in Terminating state
  and the attached volumes cannot be released/reused, the new pod will get
  stuck in ContainerCreating state, if there is no intervene from admin or
  storage software.</p>
</blockquote>

<p>Which basically means that there is no automated failover of storage, unless
you change the aforementioned option <code>Pod Deletion Policy When Node is Down</code>
to something else then <code>do-nothing</code>.</p>

<h3 id="toc_7">Testing</h3>

<p>The most important part of the high availability cluster is testing different
scenario&#39;s. Just like with backups, if you don&#39;t regularly test restores, 
you don&#39;t actually have a backup.</p>

<p>The most simple scenario is powering off one node. (<code>ssh</code>, then <code>poweroff</code>).
You should see that reflected in <code>kubectl get nodes</code>. After 5 minutes all
pods should be started on another node. Try shutting down the node that is
the <code>keepalived</code> node with <code>MASTER</code> in the HA-IP config. Note that after a
few seconds the IP will be available on the other node and you should be able
to reach any port forwards / loadbalancers again. </p>

<p>Here is a screenshot from headlamp showing one of the 
master nodes being down:</p>

<p><img src="/s/inc/img/ha-k3s-3.png" alt="headlamp"></p>

<p>The cluster should not be impacted and after 5 minutes all pods / workloads
should be back up.</p>

<p>The second test should be unplugging the network cable. Not powering off
cleanly but just unplugging the network. Same effect, after 5 minutes your
cluster should be healthy again.  </p>

<p>Third test is to power off two nodes out of three, one by one. Many more tests
are possible but this set should give you enough confidence on the
high-availability of the cluster.</p>
Tags: <a href="../tags/armbian.html">armbian</a>
, <a href="../tags/cloud.html">cloud</a>
, <a href="../tags/cluster.html">cluster</a>
, <a href="../tags/helm.html">helm</a>
, <a href="../tags/high-availability.html">high-availability</a>
, <a href="../tags/iscsi.html">iscsi</a>
, <a href="../tags/k3s.html">k3s</a>
, <a href="../tags/k8s.html">k8s</a>
, <a href="../tags/keepalived.html">keepalived</a>
, <a href="../tags/kubernetes.html">kubernetes</a>
, <a href="../tags/linux.html">linux</a>
, <a href="../tags/longhorn.html">longhorn</a>
, <a href="../tags/mariadb.html">mariadb</a>
, <a href="../tags/mysql.html">mysql</a>
, <a href="../tags/orange-pi.html">orange-pi</a>
, <a href="../tags/raspberry-pi.html">raspberry-pi</a>
, <a href="../tags/tutorials.html">tutorials</a>
</div></main>
<br/>
<footer>
<br>
                <p><small>
                <a href="/s/">Home</a> | 
                <a href="/s/static/About.html">About</a> | 
                <a href="/s/tags/all.html">All pages</a> | 
                <a href="/s/software/Sparkling_Network.html">Cluster Status</a> | 
                Generated by <a href="/s/software/ingsoc.html">ingsoc</a>.</small>
                </p>
    
    </footer>
    <script data-goatcounter="https://raymii.goatcounter.com/count"
            async src="//gc.zgo.at/count.js"></script>

    <script defer src="/s/inc/js/instant.5.2.0.js"  type="module" ></script>

     
    </main>
    </body>
    </html>
    