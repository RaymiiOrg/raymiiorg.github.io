
    <!DOCTYPE html>
    <html lang="en">
        <head>
        <title>Burn in testing for new Hypervisor and Storage server hardware - Raymii.org</title>
        <style> *, ::before, ::after {background-repeat: no-repeat;-webkit-box-sizing: border-box;box-sizing: border-box;}::before, ::after {text-decoration: inherit;vertical-align: inherit;}html {cursor: default;font-family: system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";line-height: 1.15;-moz-tab-size: 4;-o-tab-size: 4;tab-size: 4;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;word-break: break-word;}body {background-color: white;margin: 0;}h1 {font-size: 2em;margin: 0.67em 0;}hr {height: 0;overflow: visible;}main {display: block;}nav ol, nav ul {list-style: none;}pre {font-family: Roboto Mono, Menlo, Consolas, Ubuntu Monospace, Noto Mono, Oxygen Mono, Liberation Mono, monospace;font-size: 1em;}a {background-color: transparent;}abbr[title] {text-decoration: underline;-webkit-text-decoration: underline dotted;text-decoration: underline dotted;}b, strong {font-weight: bolder;}code, kbd, samp {font-family: Menlo, Consolas, Roboto Mono, Ubuntu Monospace, Noto Mono, Oxygen Mono, Liberation Mono, monospace;font-size: 1em;}small {font-size: 80%;}::-moz-selection {background-color: #b3d4fc;color: #000;text-shadow: none;}::selection {background-color: #b3d4fc;color: #000;text-shadow: none;}audio, canvas, iframe, img, svg, video {vertical-align: middle;}audio, video {display: inline-block;}audio:not([controls]) {display: none;height: 0;}img {border-style: none;}svg:not([fill]) {fill: currentColor;}svg:not(:root) {overflow: hidden;}table {border-collapse: collapse;}button, input, select, textarea {font-family: inherit;font-size: inherit;line-height: inherit;}button, input, select {margin: 0;}button {overflow: visible;text-transform: none;}button, [type="button"], [type="reset"], [type="submit"] {-webkit-appearance: button;}fieldset {padding: 0.35em 0.75em 0.625em;}input {overflow: visible;}legend {color: inherit;display: table;max-width: 100%;white-space: normal;}progress {display: inline-block;vertical-align: baseline;}select {text-transform: none;}textarea {margin: 0;overflow: auto;resize: vertical;}[type="checkbox"], [type="radio"] {padding: 0;}[type="search"] {-webkit-appearance: textfield;outline-offset: -2px;}::-webkit-inner-spin-button, ::-webkit-outer-spin-button {height: auto;}::-webkit-input-placeholder {color: inherit;opacity: 0.54;}::-webkit-search-decoration {-webkit-appearance: none;}::-webkit-file-upload-button {-webkit-appearance: button;font: inherit;}::-moz-focus-inner {border-style: none;padding: 0;}:-moz-focusring {outline: 1px dotted ButtonText;}details {display: block;}dialog {background-color: white;border: solid;color: black;display: block;height: -moz-fit-content;height: -webkit-fit-content;height: fit-content;left: 0;margin: auto;padding: 1em;position: absolute;right: 0;width: -moz-fit-content;width: -webkit-fit-content;width: fit-content;}dialog:not([open]) {display: none;}summary {display: list-item;}canvas {display: inline-block;}template {display: none;}a, area, button, input, label, select, summary, textarea, [tabindex] {-ms-touch-action: manipulation;touch-action: manipulation;}[hidden] {display: none;}[aria-busy="true"] {cursor: progress;}[aria-controls] {cursor: pointer;}[aria-disabled="true"], [disabled] {cursor: not-allowed;}[aria-hidden="false"][hidden]:not(:focus) {clip: rect(0, 0, 0, 0);display: inherit;position: absolute;}main, header, footer, article, section, aside, details, summary {margin: 0 auto;margin-bottom: 16px;width: 100%;}main {display: block;margin: 0 auto;max-width: 1000px;padding: 0 16px 16px;}footer {border-top: 1px solid rgba(0, 0, 0, 0.12);padding: 16px 0;text-align: left;}footer p {margin-bottom: 0;}hr {border: 0;border-top: 1px solid rgba(0, 0, 0, 0.12);display: block;margin-top: 16px;margin-bottom: 16px;width: 100%;-webkit-box-sizing: content-box;box-sizing: content-box;height: 0;overflow: visible;}img {height: auto;max-width: 100%;vertical-align: baseline;}@media screen and (max-width: 400px) {article, section, aside {clear: both;display: block;max-width: 100%;}img {margin-right: 16px;}}embed, iframe, video {border: 0;}body {color: rgba(0, 0, 0, 0.8);font-family: "Ubuntu", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 16px;line-height: 1.5;}p {margin: 0;margin-bottom: 16px;}h1, h2, h3, h4, h5, h6 {color: inherit;font-family: inherit;line-height: 1.2;font-weight: 500;}h1 {font-size: 40px;margin: 20px 0 16px;}h2 {font-size: 32px;margin: 20px 0 16px;}h3 {color: #75cc00;font-size: 28px;margin: 16px 0 4px;}h4 {color: #75cc00;font-size: 24px;margin: 16px 0 4px;}h5 {color: #75cc00;font-size: 20px;margin: 16px 0 4px;}h6 {color: #75cc00;font-size: 16px;margin: 16px 0 4px;}small {color: rgba(0, 0, 0, 0.54);vertical-align: bottom;}pre {background: #f7f7f9;color: rgba(0, 0, 0, 0.8);display: block;font-family: "Roboto Mono", Menlo, Monaco, Consolas, "Courier New", monospace;font-size: 16px;margin: 16px 0;padding: 16px;white-space: pre-wrap;overflow-wrap: break-word;}code {background: #f7f7f9;color: rgba(0, 0, 0, 0.8);font-family: "Roboto Mono", Menlo, Monaco, Consolas, "Courier New", monospace;font-size: 16px;line-height: inherit;margin: 0;vertical-align: baseline;word-break: break-all;word-wrap: break-word;}a {color: #75cc00;text-decoration: none;background-color: transparent;}a:hover, a:focus {color: #0062cc;font-weight: bolder;text-decoration: underline;}dl {margin-bottom: 16px;}dd {margin-left: 40px;}ul, ol {margin-bottom: 8px;padding-left: 40px;vertical-align: baseline;}blockquote {border-left: 2px solid rgba(0, 0, 0, 0.8);font-family: Georgia, Times, "Times New Roman", serif;font-style: italic;margin: 16px 0;padding-left: 16px;}figcaption {font-family: Georgia, Times, "Times New Roman", serif;}u {text-decoration: underline;}s {text-decoration: line-through;}sup {font-size: 14px;vertical-align: super;}sub {font-size: 14px;vertical-align: sub;}mark {background: #ffeb3b;}input[type="text"], input[type="password"], input[type="email"], input[type="url"], input[type="date"], input[type="month"], input[type="time"], input[type="datetime"], input[type="datetime-local"], input[type="week"], input[type="number"], input[type="search"], input[type="tel"], select, textarea {background: #fff;background-clip: padding-box;border: 1px solid rgba(0, 0, 0, 0.12);border-radius: 4px;color: rgba(0, 0, 0, 0.8);display: block;width: 100%;padding: 8px 16px;line-height: 1.5;-webkit-transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";}input[type="color"] {background: #fff;border: 1px solid rgba(0, 0, 0, 0.12);border-radius: 4px;display: inline-block;vertical-align: middle;}input:not([type]) {-webkit-appearance: none;background: #fff;background-clip: padding-box;border: 1px solid rgba(0, 0, 0, 0.12);border-radius: 4px;color: rgba(0, 0, 0, 0.8);display: block;width: 100%;padding: 8px 16px;line-height: 1.5;-webkit-transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;text-align: left;}input[type="text"]:focus, input[type="password"]:focus, input[type="email"]:focus, input[type="url"]:focus, input[type="date"]:focus, input[type="month"]:focus, input[type="time"]:focus, input[type="datetime"]:focus, input[type="datetime-local"]:focus, input[type="week"]:focus, input[type="number"]:focus, input[type="search"]:focus, input[type="tel"]:focus, input[type="color"]:focus, select:focus, textarea:focus {background-color: #fff;border-color: #80bdff;outline: 0;-webkit-box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);}input:not([type]):focus {background-color: #fff;border-color: #80bdff;outline: 0;-webkit-box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);}input[type="file"]:focus, input[type="radio"]:focus, input[type="checkbox"]:focus {outline: 1px thin rgba(0, 0, 0, 0.12);}input[type="text"][disabled], input[type="password"][disabled], input[type="email"][disabled], input[type="url"][disabled], input[type="date"][disabled], input[type="month"][disabled], input[type="time"][disabled], input[type="datetime"][disabled], input[type="datetime-local"][disabled], input[type="week"][disabled], input[type="number"][disabled], input[type="search"][disabled], input[type="tel"][disabled], input[type="color"][disabled], select[disabled], textarea[disabled] {background-color: rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.54);cursor: not-allowed;opacity: 1;}input:not([type])[disabled] {background-color: rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.54);cursor: not-allowed;opacity: 1;}input[readonly], select[readonly], textarea[readonly] {border-color: rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.54);}input:focus:invalid, textarea:focus:invalid, select:focus:invalid {border-color: #ea1c0d;color: #f44336;}input[type="file"]:focus:invalid:focus, input[type="radio"]:focus:invalid:focus, input[type="checkbox"]:focus:invalid:focus {outline-color: #f44336;}select {border: 1px solid rgba(0, 0, 0, 0.12);vertical-align: sub;}select:not([size]):not([multiple]) {height: -webkit-calc(2.25rem + 2px);height: calc(2.25rem + 2px);}select[multiple] {height: auto;}label {display: inline-block;line-height: 2;}fieldset {border: 0;margin: 0;padding: 8px 0;}legend {border-bottom: 1px solid rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.8);display: block;margin-bottom: 8px;padding: 8px 0;width: 100%;}textarea {overflow: auto;resize: vertical;}input[type=checkbox], input[type=radio] {-webkit-box-sizing: border-box;box-sizing: border-box;padding: 0;display: inline;}input[type=submit], input[type=reset], input[type=button], button {background-color: #75cc00;border: #75cc00;border-radius: 4px;color: #fff;padding: 8px 16px;display: inline-block;font-weight: 400;text-align: center;white-space: nowrap;vertical-align: middle;-webkit-user-select: none;-moz-user-select: none;-ms-user-select: none;user-select: none;border: 1px solid transparent;font-size: 1rem;line-height: 1.5;-webkit-transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, box-shadow .15s ease-in-out;transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, box-shadow .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;}input[type=submit]::-moz-focus-inner, input[type=reset]::-moz-focus-inner, input[type=button]::-moz-focus-inner, button::-moz-focus-inner {padding: 0;}input[type=submit]:hover, input[type=reset]:hover, input[type=button]:hover, button:hover {background-color: #0069d9;border-color: #0062cc;color: #fff;}input[type=submit]:not(:disabled):active, input[type=reset]:not(:disabled):active, input[type=button]:not(:disabled):active, button:not(:disabled):active {background-color: #0062cc;border-color: #005cbf;color: #fff;}input[type=submit]:focus, input[type=reset]:focus, input[type=button]:focus, button:focus {outline: 0;-webkit-box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.5);box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.5);}input[type=submit]:disabled, input[type=reset]:disabled, input[type=button]:disabled, button:disabled {opacity: .65;cursor: not-allowed;background-color: #75cc00;border-color: #75cc00;color: #fff;}table {border-top: 1px solid rgba(0, 0, 0, 0.12);margin-bottom: 16px;}caption {padding: 8px 0;}thead th {border: 0;border-bottom: 2px solid rgba(0, 0, 0, 0.12);text-align: left;}tr {margin-bottom: 8px;}th, td {border-bottom: 1px solid rgba(0, 0, 0, 0.12);padding: 16px;white-space: nowrap;vertical-align: inherit;}tfoot tr {text-align: left;}tfoot td {color: rgba(0, 0, 0, 0.54);font-size: 8px;font-style: italic;padding: 16px 4px;}a.skip-main {left:-999px;position:absolute;top:auto;width:1px;height:1px;overflow:hidden;z-index:-999;}a.skip-main:focus, a.skip-main:active {color: #fff;background-color:#000;left: auto;top: auto;width: 30%;height: auto;overflow:auto;margin: 10px 35%;padding:5px;border-radius: 15px;border:4px solid yellow;text-align:center;font-size:1.2em;z-index:999;}@font-face {font-family: 'Raleway';font-style: normal;font-weight: 600;src: url('/s/inc/css/raleway-v18-latin-600.eot');src: local(''), url('/s/inc/css/raleway-v18-latin-600.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/raleway-v18-latin-600.woff2') format('woff2'), url('/s/inc/css/raleway-v18-latin-600.woff') format('woff'), url('/s/inc/css/raleway-v18-latin-600.ttf') format('truetype'), url('/s/inc/css/raleway-v18-latin-600.svg#Raleway') format('svg');}@font-face {font-family: 'Raleway';font-style: italic;font-weight: 400;src: url('/s/inc/css/raleway-v18-latin-italic.eot');src: local(''), url('/s/inc/css/raleway-v18-latin-italic.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/raleway-v18-latin-italic.woff2') format('woff2'), url('/s/inc/css/raleway-v18-latin-italic.woff') format('woff'), url('/s/inc/css/raleway-v18-latin-italic.ttf') format('truetype'), url('/s/inc/css/raleway-v18-latin-italic.svg#Raleway') format('svg');}@font-face {font-family: 'Roboto Mono';font-style: normal;font-weight: 400;src: url('/s/inc/css/roboto-mono-v12-latin-regular.eot');src: local(''), url('/s/inc/css/roboto-mono-v12-latin-regular.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/roboto-mono-v12-latin-regular.woff2') format('woff2'), url('/s/inc/css/roboto-mono-v12-latin-regular.woff') format('woff'), url('/s/inc/css/roboto-mono-v12-latin-regular.ttf') format('truetype'), url('/s/inc/css/roboto-mono-v12-latin-regular.svg#RobotoMono') format('svg');}@font-face {font-family: 'Roboto Mono';font-style: normal;font-weight: 600;src: url('/s/inc/css/roboto-mono-v12-latin-600.eot');src: local(''), url('/s/inc/css/roboto-mono-v12-latin-600.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/roboto-mono-v12-latin-600.woff2') format('woff2'), url('/s/inc/css/roboto-mono-v12-latin-600.woff') format('woff'), url('/s/inc/css/roboto-mono-v12-latin-600.ttf') format('truetype'), url('/s/inc/css/roboto-mono-v12-latin-600.svg#RobotoMono') format('svg');}@font-face {font-family: 'Ubuntu';font-style: normal;font-weight: 400;src: url('/s/inc/css/ubuntu-v15-latin-regular.eot');src: local(''), url('/s/inc/css/ubuntu-v15-latin-regular.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/ubuntu-v15-latin-regular.woff2') format('woff2'), url('/s/inc/css/ubuntu-v15-latin-regular.woff') format('woff'), url('/s/inc/css/ubuntu-v15-latin-regular.ttf') format('truetype'), url('/s/inc/css/ubuntu-v15-latin-regular.svg#Ubuntu') format('svg');}@font-face {font-family:'Raleway2';font-style:normal;font-weight:normal;src:url('/s/inc/css/raleway.eot');src:local('Raleway2'),local('Raleway2'),url('/s/inc/css/raleway.ttf') }.headheader {font-family:"Raleway2"!important }.headheader a {color:#000;text-decoration:none }.headheader a:hover {color:#000;text-decoration:none!important }#toc ul {list-style: none;margin: 0;padding: 0;}#toc h3 {color:black;}</style>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link type="application/opensearchdescription+xml" rel="search" href="/s/inc/opensearch.xml"/>
        <link rel="alternate" type="application/rss+xml" title="RSS Feed for Raymii.org" href="https://raymii.org/s/feed.xml" />
    </head>
    <body>
        
        <a id="top-of-page"></a>
        <main>
        <a class="skip-main" href="#main">Skip to main content</a>
            <header>
                <h1 class="headheader">
                    <a href="https://raymii.org/s/">Raymii.org 
                        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAKCAYAAAD2Fg1xAAABgElEQVQ4jb3VP0iVYRTH8c9waXBokog7OYhTXChuF3GIi4hoiJA4REQIOTgGtoWTg6ODs0SYComIXCJEMhpKtD9guUU0ujRFS0PQ8DzC24v3Pq+3S9/pnMOP8/7Ocx6el/OziRN0JXTD+I2xhK4WdeNteGmbu8IgC3jQQlfCZ0zgINHzJabwoQP+ClHGV1zGJXwRDJ/FDJZi3MBQE10dL2K8gZFOGE3REDZyyjLunKG7KAzZHfMaXjXp+QbXYlzBfrvmSuhBNaHrxQU8zdQW8RhrOe0snuB7zA/jd6p4n9HV8QMfY/4JPzGAt7meFfS18LdXEk7uemIQuJ/Lj6PZQezFWhm3cTWnXcAj3MrU5oWh5WpzGM3UurGNZy28HSa8J7mB3Uy+4u/rl+UdrsT4Jraa6F6jP5M3MP0PHguzL9zzqmC2GRNYjXF2qDzDwgbgHp53wGMhJrEunGQ9oT3CQ+GFasWBsLVvwiv5XygJz/JOAe208POrJHST+CVspBB/AFY9Q3+QJqLxAAAAAElFTkSuQmCC" alt="Raymii.org Logo">
                    </a>
                </h1>
                <small>
                  Quis custodiet ipsos custodes?<br>
                  <a href="/s/">Home</a> | 
                  <a href="/s/static/About.html">About</a> | 
                  <a href="/s/tags/all.html">All pages</a> | 
                  <a href="/s/software/Sparkling_Network.html">Cluster Status</a> | 
                  <a href="https://raymii.org/s/feed.xml">RSS Feed</a> | 
                  <a href="gopher://raymii.org:70">Gopher</a>
                </small>
            </header>
          
    <h2 class='headheader' id='main'>Burn in testing for new Hypervisor and Storage server hardware</h2>
<p><small>Published: 08-04-2017 | Author: Remy van Elst | <a href="Burn_in_testing_for_Hypervisor_and_Storage_servers.txt">Text only version of this article</a>
</small></p>
<div class='ad'>
                                <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7993642564731324"
                             crossorigin="anonymous"></script>
                        <!-- voorartikel -->
                        <ins class="adsbygoogle"
                             style="display:block"
                             data-ad-client="ca-pub-7993642564731324"
                             data-ad-slot="6172324376"
                             data-ad-format="auto"></ins>
                        <script>
                             (adsbygoogle = window.adsbygoogle || []).push({});
                        </script>
                        </div><br><div class='olderthanayear'><p><strong>&#10071; This post is over four years old. It may no longer be up to date. Opinions may have changed.</strong></p></div>
<div id="toc">
<h3>Table of Contents</h3>
<ul>
<li>
<a href="#toc_0">Preface</a>
</li>
<li>
<a href="#toc_1">The Problem</a>
</li>
<li>
<a href="#toc_2">Simulating usage patterns</a>
</li>
<li>
<a href="#toc_3">Compute servers: stress, stress-ng and other tools</a>
<ul>
<li>
<a href="#toc_4">Out of memory</a>
</li>
<li>
<a href="#toc_5">Install packages</a>
</li>
<li>
<a href="#toc_6">CPU</a>
</li>
<li>
<a href="#toc_7">Memory</a>
</li>
<li>
<a href="#toc_8">Disk</a>
</li>
<li>
<a href="#toc_9">Network</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">Storage servers: dd, dd and some more dd</a>
</li>
<li>
<a href="#toc_11">The result</a>
</li>
</ul>

</div><hr><div id="contents">
<p>This article talks over how and why to do burn in testing on hypervisor and
storage servers. I work at a fairly large cloud provider, where we have a lot of
hardware. Think thousands of hardware servers and multiple ten thousand
harddisks. It&#39;s all technology, so stuff breaks, and at our scale, stuff breaks
often. One of my pet projects for the last period has been to automate the burn-
in testing for our virtualisation servers and the storage machines. We run
OpenStack and use KVM for the hypervisors and a combination of different storage
technology for the volume storage servers. Before they go in production, they
are tested for a few days with very intensive automated usage. We&#39;ve noticed
that they either fail then, or not. This saves us from having to migrate
customers off of new production servers just a few days after they&#39;ve gone live.
The testing is of course all automated.</p>

<p class="ad"> <a href="https://leafnode.nl">I'm developing a desktop monitoring app,  Leaf Node Monitoring, open source, but paid. For Windows, Linux & Android, go check it out.</a><br><br> <a href="https://github.com/sponsors/RaymiiOrg/">Consider sponsoring me on Github. It means the world to me if you show your appreciation and you'll help pay the server costs.</a><br><br> <a href="https://www.digitalocean.com/?refcode=7435ae6b8212">You can also sponsor me by getting a Digital Ocean VPS. With this referral link you'll get $100 credit for 60 days. </a><br><br> </p>

<p><img src="https://raymii.org/s/inc/img/busy-compute.jpg" alt=""></p>

<blockquote>
<p>A very busy hypervisor node</p>
</blockquote>

<h3 id="toc_0">Preface</h3>

<p>This article is not a copy and paste tutorial. It&#39;s more a walkthrough of the
processes used and the thought process behind it.</p>

<p>As said, I currently work at an OpenStack public cloud provider. That means that
you can order a virtual server with us and get full administrative access to it.
We use OpenStack, so you can also automate that part using the API and deploy
instances automatically. Those virtual machines do need to run on actual
hardware, which is where this article comes in.</p>

<p><img src="https://raymii.org/s/inc/img/automate-all-the-things.png" alt=""></p>

<p>The regular process for deploying new hardware is fully automated. We&#39;ve got
capacity management nailed down, once the OpenStack environment reaches a
certain threshold, PDF&#39;s are automatically generated with investment requests
and sent off to the finance department and the hardware is ordered. Then after
some time, the nodes are shipped to the datacenters. Our datacenter team handles
the racking and stacking. They setup the remote out of band management (ILO,
iDrac, IPMI) and put the credentials into the PXE deployment servers (MaaS).</p>

<p>The machine get&#39;s installed with the required OS automatically and after that
the software stack (OpenStack) is installed. The machine firmwares are then
updated to the latest versions, the rest of the cluster configuration is updated
and it&#39;s ready to go. This is all done using Ansible, after the racking and
stacking no human is involved. The only thing I have to do is to enable the
<code>nova</code> or <code>cinder</code> service in OpenStack for that machine, and it&#39;s ready to go.</p>

<p>The machine is automatically put into our monitoring system as well. We not only
monitor the software side of things, like cpu load, disk usage, network
connections, required services running, but also the hardware itself. Either via
the remote out of band management or vendor provided tools (<code>omreport</code> anyone?).
Which means that when a disk breaks, or a faulty memory module is found, our
monitoring system alerts us and takes action automatically. When defective disks
are detected, for example, the vendor automatically gets an RMA sent from our
monitoring. Once a week a bunch of disks arrive at the office or the data
center, depending on the vendor, and the datacenter team replace the faulty
ones. Even the list with disks to replace by that team it automatically sent
from the monitoring.</p>

<p>This level of automation is required when you reach a scale like this. By
automating all of this, our sysadmin team can focus on other things than the
gruntwork of installing software or ordering hardware. This level of automation
and monitoring also provides a layer to build stuff on top of, which we will be
doing here.</p>

<h3 id="toc_1">The Problem</h3>

<p>Stuff breaks. It&#39;s technology, so just like your car, things break. That&#39;s not a
problem if you&#39;ve built your environment redundantly and highly available, but
my experience is that not a lot of people do that.</p>

<p>Hardware breakage usually doesn&#39;t mean downtime right away. Most parts are
redundant. Multiple network interfaces that are bonded. Hard disks are in a form
of RAID. Multiple PSU&#39;s on different power feeds. Multiple CPU&#39;s. Same goes for
the network and other hardware. If a NIC fails, the bond will make sure the
system keeps working. A drive dies? One of the spares is automatically put in
the vdev or array and is rebuilt. Power goes out or a PSU blows? The other feed
keeps running. However, it does mean that the faulty part needs to be replaced
and possibly that customers that have instances or storage running on the
hypervisor need to be migrated off there.</p>

<p>Migrating instances and storage is something that we do a lot. Not just when
there are problems with hardware, also for regular maintenance. Servers need
updates to their software and firmware. The updates are done with Ansible and
are automated. We&#39;ve written software that checks if there is enough cluster
capacity and no issues in the monitoring. If so, it checks if a node had
firmware updates outstanding, or more than 50 packages or a security package
update outstanding, and if so it schedules a node for emptying out.</p>

<p>We use live migrates and that process it automated as well, but it does add
extra workload. Especially larger Windows VM&#39;s on KVM tend to lock up and break
during a live-migrate, those need some tender love and care. Migrating storage
(OpenStack Cinder Volumes) goes fine, never had any issues with that.</p>

<p>Depending on the used configuration, OpenStack can use local storage or Ceph.
With Ceph, live migrated of Volumes and Instances is very easy and fast. Local
storage takes longer, since then the disks need to be copied over.</p>

<p>Once the node is empty, it&#39;s updated, both software and firmware, with the same
playbooks we use when a new node is installed. When that&#39;s done, it&#39;s rebooted
and enabled back in OpenStack. This process is done a few times a day, making
sure all the hardware is updated regularly. Due to our environment getting
bigger and bigger, it takes longer to update all nodes, so we are testing if we
can do more than one node at a time.</p>

<p>New hardware, or hardware that has had issues and got replacement parts, tends
to break more often than hardware that we have in use for a few months. Mostly
hard disks or memory modules (ECC RAM), but I&#39;ve also seen PSU&#39;s blow, and once
we had drops of solder breaking off a fibre channel NIC and causing a short. As
said, it&#39;s technology, so stuff breaks. All covered by warranty, so not a big
problem.</p>

<h3 id="toc_2">Simulating usage patterns</h3>

<p>New hardware, or hardware that is put back into production after a repair wasn&#39;t
getting a burn in test beforehand. Mostly because the amount of problems we
experienced was small, just a one off most of the time. Now we are a lot larger,
almost every week there is new hardware to be racked, so we see more problems.
Just a scale issue, nothing to worry about. I suspect that when you are a large
car shop you get more faulty returns than when your a local garage around the
corner.</p>

<p>Since we saw that hardware can break when it&#39;s being used regularly, we also
must test it using mostly regular usage patterns. For compute nodes that means,
install OpenStack and run VM&#39;s. For storage machines, that means, install
OpenStack and the specific storage (ZFS, Ceph, Linux LVM) and generate IO.</p>

<p>Because we do want to stress the node a bit, we generate usage that under normal
conditions would count as abuse. We thought of bitcoin miners or BOINC
(SETI@HOME), but decided that that wouldn&#39;t be reproducable enough. Therefore we
went with regular tools, like <code>stress</code>, <code>stress-ng</code>, <code>memtest</code>, <code>dd</code> and
<code>iperf</code>.</p>

<p>Using OpenStack Userdata we provide a script to the instance that installs the
packages and runs them in a specified order for a set amount of time. In my
example every tool, testing a specific aspect (CPU, RAM, etc) runs for 15
minutes and then continues on to the next part. By creating a VM every 10
minutes, all usage patterns are equal. With that I mean that the CPU isn&#39;t
hammered for 15 minutes, then the RAM, then the NIC. No, one instance is
hammering the disk while another is using all it&#39;s CPU.</p>

<p>The below image shows <code>htop</code> on one of our empty compute nodes:</p>

<p><img src="https://raymii.org/s/inc/img/empty-compute.jpg" alt=""></p>

<p>One thing we do not test enough is a huge amount of small VM&#39;s, thus having a
lot of hardware interrupts and context switching. My testing uses a few large
instances, which, in our case, tests the stuff we need. These burn in tests have
saved us over two dozen nodes with issues going into production in the first
three months of using this new procedure. In saved-man-hours on RMA and
replacement that&#39;s almost three people fulltime for a week. Huge cost-savings.</p>

<p>Let&#39;s continue on to the actual testing to see how much this node can be
hammered.</p>

<h3 id="toc_3">Compute servers: stress, stress-ng and other tools</h3>

<p>For the compute node benchmarking I&#39;m using an Ubuntu 14.04 VM. I&#39;m not able to
share the Ansible playbooks we are using, but I can give you the manual
commands. It&#39;s not that hard to put one and one together and create your own
version. Or, if you don&#39;t get new hardware so often, just do it manually.</p>

<p>In our specific OpenStack deployments we have an administrative user. We need
this to override the scheduler and control on which compute node the VM&#39;s get
spawned. We also have a special flavor with no IOPS or other usage limits.
Regular customer instances have CPU, network and iops limits set and that we
don&#39;t want in this case.</p>

<p>By specifing the specific node in the <code>nova boot</code> command, we can force a VM to
get spawned on that hypervisor:</p>

<pre><code>nova boot --flavor=&quot;nolimit&quot; --image &quot;Ubuntu 14.04&quot; --nic net-id=00000000-0000-0000-0000-000000000000  --availability-zone NL1:compute-x-y
</code></pre>

<p>This does allow you to overload the hypervisor with instances. In our case it
either cannot boot them and the logs say, cannot allocate memory. Or, the out-
of-memory killer on the hypervisor just kills the <code>qemu</code> process and the
instance is in state stopped. The nova instance action list doesn&#39;t show that
action (because it went outside of nova).</p>

<h4 id="toc_4">Out of memory</h4>

<p>Inside the instances it&#39;s important to disable the linux OOM killer. Otherwise
it will stop your stress tests. You can do this on the hypervisor as well, but
that might have unexpected side effects. Make sure you have out of band access
so that you can reboot a node when it has hung itself up to a tree.</p>

<p>Here&#39;s how to disable the OOM killer inside of your VM:</p>

<pre><code>sysctl vm.overcommit_memory=2
</code></pre>

<p>If you want it to survive a reboot, place it in <code>sysctl.conf</code>:</p>

<pre><code>echo &quot;vm.overcommit_memory=2&quot; &gt;&gt; /etc/sysctl.conf
</code></pre>

<h4 id="toc_5">Install packages</h4>

<p>The software we&#39;re using is all in the Ubuntu repositories. Install the
packages:</p>

<pre><code>apt-get -y install vnstat memtester stress stress-ng iperf
</code></pre>

<h4 id="toc_6">CPU</h4>

<p>The first part takes the CPU and uses <code>stress</code> to generate usage:</p>

<pre><code>stress --cpu $(nproc) --timeout 900
</code></pre>

<p><code>nproc</code> gives me the number of cores available, which is usable for the VM.</p>

<p>Stress with the <code>--cpu</code> parameter spins up processes in a tight loop calculating
the <code>sqrt()</code> of a random number acquired with <code>rand()</code>.</p>

<h4 id="toc_7">Memory</h4>

<p>For RAM, I found <code>stress</code> not getting that much result as its modern counterpart
<code>stress-ng</code> did. <code>stress</code> doesn&#39;t have an option to utilize the RAM, <code>stress-ng</code>
does:</p>

<pre><code>stress-ng -vm 60 --vm-bytes 1G -t 900s --vm-method zero-one  --metrics-brief
</code></pre>

<p>As per the manpage, <code>-vm</code> starts N workers continuously calling
mmap(2)/munmap(2) and writing to the allocated memory. <code>--vm-method zero-one</code>
sets all memory bits to zero and then checks if any bits are not zero. Next, set
all the memory bits to one and check if any bits are not one. Simple but
effective test. My instance has 64 GB RAM so 60 workers using 1 GB each will
fill the RAM up nicely.</p>

<p>This memory abuse has triggered a lot of servers where one of the RAM DIMM&#39;s was
bad. Even though we always have ECC RAM. Only after a few hours of running, not
right away.</p>

<h4 id="toc_8">Disk</h4>

<p>In our setup Ceph is mostly used, but we do also have local disks or volume
servers (boot from volume) with ZFS. When a compute node has local disks we test
them, otherwise it isn&#39;t much use since the storage servers are stressed
differently.</p>

<p><code>stress</code> has two different use cases for disk IO. The first does actual writes.</p>

<pre><code>stress --hdd $(nproc) --hdd-bytes 100G --timeout 900 
</code></pre>

<p><code>--hdd</code> spawns N workers spinning on <code>write()</code>/<code>unlink()</code>. <code>--hdd-bytes</code> write B
bytes per hdd worker (default is 1GB).</p>

<pre><code>stress --io $(nproc) --timeout 900 
</code></pre>

<p><code>--io</code> spawns N workers spinning on <code>sync()</code>.</p>

<p>Below is a screenshot of <code>iotop</code> on the compute node when a burn-in test is
running on disk io with these tests:</p>

<p><img src="https://raymii.org/s/inc/img/iops-compute.jpg" alt=""></p>

<p>There have been four SSD&#39;s so far that needed replacing when these tests ran for
a day. On average that suprised me, since I suspected it to be a lot more.</p>

<h4 id="toc_9">Network</h4>

<p>For the sake of testing we also do a speedtest to an internal <code>iperf</code> server. I
haven&#39;t seen any network cards fail yet. It is however nice that we can utilize
the network card to it&#39;s fullest potential. I did find one card that had been
configured in 10 mbit half-duplex mode because of a bad cable while running this
test. However I was looking at it then, that&#39;s not something the monitoring
tools report on (yet):</p>

<pre><code>iperf --port 5002 --client speedtest.serverius.net --dualtest --time 900 --format M 
</code></pre>

<p>I replaced our <code>iperf</code> server address with a public <code>iperf</code> server address.</p>

<h3 id="toc_10">Storage servers: dd, dd and some more dd</h3>

<p>Our storage servers are tested by doing a lot of disk activity. Since we do want
to simulate actual usage, we install them as OpenStack Cinder servers and create
volumes on them. Those volumes are then attached to an instance. On the
instance, using <code>zfs</code>, we create a big pool (named <code>tank</code>) (no <code>raidZ</code> or
mirroring) and one dataset (named <code>bullet</code>). Then, using <code>dd</code>, we first write
zero&#39;s for a few hours. Afterwards, also using <code>dd</code> and random data from
<code>openssl</code> (<code>/dev/urandom</code> is too slow), actual data is written.</p>

<p>This could of course also be done with regular LVM volumes or <code>mdraid</code>. The
tests however are adapted from work I already had laying around, that playbook
already did <code>zfs</code>, so not much use in re-inventing the wheel.</p>

<p>I&#39;m using Ubuntu 16.04 because of the ZFS support in there. First install it:</p>

<pre><code>apt-get install zfsutils-linux zfs
</code></pre>

<p>Using <code>parted</code> with <code>ansible</code>, create a <code>gpt</code> partition:</p>

<pre><code>- name: partition gpt
  parted:
    device: &quot;{{ item }}&quot;
    label: gpt
    state: present
  with_items:
    - /dev/sdb 
    - /dev/sdc 
    - /dev/sdd 
    - /dev/...
</code></pre>

<p>Create the <code>zpool</code> and the <code>dataset</code> (again, with Ansible):</p>

<pre><code>- name: Create the zpool     
  shell: |       
    zpool create tank /dev/sdb /dev/sdc /dev/sdd /dev/... &amp;&amp; \ 
    zfs set compression=lz4 tank &amp;&amp; \       
    zfs set atime=off tank     

- name: Create fs
  zfs:
    name: tank/bullet
    state: present
</code></pre>

<p>It will be automatically mounted and available under <code>/tank/bullet</code>.</p>

<p>The first <code>dd</code> command:</p>

<pre><code>dd if=/dev/zero of=/tank/bullet/bench bs=20000M count=1024 conv=fdatasync,notrunc
</code></pre>

<p>In my case the pool is 20 TB. Change where needed. Writing zero&#39;s will go full
speed. If you want to test actual data writing, and thus bypass any cache or
raid controllers, you need to generate random data. <code>/dev/random</code> and
<code>/dev/urandom</code> are way to slow. I&#39;ve found the following command to get
acceptable speeds:</p>

<pre><code>openssl enc -aes-256-ctr -pass pass:&quot;$(dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64)&quot; -nosalt &lt; /dev/zero &gt; /tank/bullet/bench
</code></pre>

<p>This will use the <code>AES-NI</code> CPU instruction if your VM supports it. Let this run
for a few hours and you will have all your disks written to. Do note that a full
<code>zfs</code> will be extremely slow and behave weirdly. Throw away the VM and boot up a
new one, rinse and repeat.</p>

<h3 id="toc_11">The result</h3>

<p><img src="https://raymii.org/s/inc/img/iops-zfs.png" alt=""></p>

<blockquote>
<p>Writing zero&#39;s at near-line speed</p>
</blockquote>

<p><img src="https://raymii.org/s/inc/img/busy-compute.jpg" alt=""></p>

<blockquote>
<p>A busy hypervisor node</p>
</blockquote>

<p><img src="https://raymii.org/s/inc/img/busy-compute2.jpg" alt=""></p>

<blockquote>
<p>An even more busy hypervisor node</p>
</blockquote>

<p>You might get crashing servers or overheated switches. At one of my previous
employers we actually found out that the fan rig wasn&#39;t correctly wired due to
CPU heat alarms going off in the <code>ipmi</code>. Once again, make sure you have good
monitoring and out of band access. This will happen often:</p>

<p><img src="https://raymii.org/s/inc/img/notification-memory.png" alt=""></p>

<p>Which is of course a good thing when the hardware is not yet in use.</p>
Tags: <a href="../tags/blog.html">blog</a>
, <a href="../tags/cinder.html">cinder</a>
, <a href="../tags/cloud.html">cloud</a>
, <a href="../tags/compute.html">compute</a>
, <a href="../tags/nova.html">nova</a>
, <a href="../tags/openstack.html">openstack</a>
, <a href="../tags/storage.html">storage</a>
</div>
<br/>
<footer>
<br/>
                <form role="search" action="https://encrypted.google.com/search"
                style="min-width:250px;max-width:300px;">
                    <div class="form-group">
                      <input type="hidden" name="as_sitesearch" value="raymii.org">
                      <input type="hidden" name="as_qdr" value="all">
                      <input type="text" name="as_q" class="form-control" placeholder="Search">
                    </div>
                  </form>
                <br>
                <p><small>
                <a href="/s/">Home</a> | 
                <a href="/s/static/About.html">About</a> | 
                <a href="/s/tags/all.html">All pages</a> | 
                <a href="/s/software/Sparkling_Network.html">Cluster Status</a> | 
                Generated by <a href="/s/software/ingsoc.html">ingsoc</a>.</small>
                </p>
    
    </footer>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-3704876-6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-3704876-6');
    </script>
    <script data-goatcounter="https://raymii.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
     
    </main>
    </body>
    </html>
    