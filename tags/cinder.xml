<?xml version="1.0" ?>
    <rss version="2.0"  xmlns:atom="http://www.w3.org/2005/Atom">
        <channel>
            <title>RSS feed for tag cinder on Raymii.org</title> 
            <link>https://raymii.org/s/tags/cinder.xml</link> 
            <description>RSS feed for tag cinder on Raymii.org</description>
            <atom:link href="https://raymii.org/s/tags/cinder.xml" rel="self" type="application/rss+xml" />
    
            <item>
                <title>Burn in testing for new Hypervisor and Storage server hardware</title> 
                <link>https://raymii.org/s/blog/Burn_in_testing_for_Hypervisor_and_Storage_servers.html?utm_medium=rss&amp;utm_source=raymii&amp;utm_campaign=tagrss</link> 
                <guid>https://raymii.org/s/blog/Burn_in_testing_for_Hypervisor_and_Storage_servers.html</guid>
                <description>This article talks over how and why to do burn in testing on hypervisor and storage servers. I work at a fairly large cloud provider, where we have a lot of hardware. Think thousands of hardware servers and multiple ten thousand harddisks. It's all technology, so stuff breaks, and at our scale, stuff breaks often. One of my pet projects for the last period has been to automate the burn-in testing for our virtualisation servers and the storage machines. We run OpenStack and use KVM for the hypervisors and a combination of different storage technology for the volume storage servers. Before they go in production, they are tested for a few days with very intensive automated usage. We've noticed that they either fail then, or not. This saves us from having to migrate customers off of new production servers just a few days after they've gone live. The testing is of course all automated.</description> 
                <pubDate>Sat, 08 Apr 2017 00:00:00 GMT</pubDate>
                <lastBuildDate>Sat, 08 Apr 2017 00:00:00 GMT</lastBuildDate>
            </item>
    
            <item>
                <title>Fix inconsistent Openstack volumes and instances from Cinder and Nova via the database</title> 
                <link>https://raymii.org/s/articles/Fix_inconsistent_Openstack_volumes_and_instances_from_Cinder_and_Nova_via_the_database.html?utm_medium=rss&amp;utm_source=raymii&amp;utm_campaign=tagrss</link> 
                <guid>https://raymii.org/s/articles/Fix_inconsistent_Openstack_volumes_and_instances_from_Cinder_and_Nova_via_the_database.html</guid>
                <description>When running Openstack, sometimes the state of a volume  or an instance can be inconsistent on the cluster. Nova might find a volume attached while Cinder says the volume is detached or otherwise. Sometimes a volume deletion hangs, or a detach does not work. If you've found and fixed the underlying issue (lvm, iscsi, ceph, nfs etc...) you need to bring the database up to date with the new consistent state. Most of the time a reset-state works, sometimes you need to manually edit the database to correct the state. These snippets show you how.</description> 
                <pubDate>Mon, 22 Dec 2014 00:00:00 GMT</pubDate>
                <lastBuildDate>Mon, 22 Dec 2014 00:00:00 GMT</lastBuildDate>
            </item>
    
        </channel>
    </rss>
    
    