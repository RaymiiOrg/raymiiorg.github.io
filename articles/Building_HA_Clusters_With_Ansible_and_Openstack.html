
    <!DOCTYPE html>
    <html lang="en">
        <head>
        <title>Building HA Clusters with Ansible and Openstack - Raymii.org</title>
        <style> *, ::before, ::after {background-repeat: no-repeat;-webkit-box-sizing: border-box;box-sizing: border-box;}::before, ::after {text-decoration: inherit;vertical-align: inherit;}html {cursor: default;font-family: system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";line-height: 1.15;-moz-tab-size: 4;-o-tab-size: 4;tab-size: 4;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;word-break: break-word;}body {background-color: white;margin: 0;}h1 {font-size: 2em;margin: 0.67em 0;}hr {height: 0;overflow: visible;}main {display: block;}nav ol, nav ul {list-style: none;}pre {font-family: Roboto Mono, Menlo, Consolas, Ubuntu Monospace, Noto Mono, Oxygen Mono, Liberation Mono, monospace;font-size: 1em;}a {background-color: transparent;}abbr[title] {text-decoration: underline;-webkit-text-decoration: underline dotted;text-decoration: underline dotted;}b, strong {font-weight: bolder;}code, kbd, samp {font-family: Menlo, Consolas, Roboto Mono, Ubuntu Monospace, Noto Mono, Oxygen Mono, Liberation Mono, monospace;font-size: 1em;}small {font-size: 80%;}::-moz-selection {background-color: #b3d4fc;color: #000;text-shadow: none;}::selection {background-color: #b3d4fc;color: #000;text-shadow: none;}audio, canvas, iframe, img, svg, video {vertical-align: middle;}audio, video {display: inline-block;}audio:not([controls]) {display: none;height: 0;}img {border-style: none;}svg:not([fill]) {fill: currentColor;}svg:not(:root) {overflow: hidden;}table {border-collapse: collapse;}button, input, select, textarea {font-family: inherit;font-size: inherit;line-height: inherit;}button, input, select {margin: 0;}button {overflow: visible;text-transform: none;}button, [type="button"], [type="reset"], [type="submit"] {-webkit-appearance: button;}fieldset {padding: 0.35em 0.75em 0.625em;}input {overflow: visible;}legend {color: inherit;display: table;max-width: 100%;white-space: normal;}progress {display: inline-block;vertical-align: baseline;}select {text-transform: none;}textarea {margin: 0;overflow: auto;resize: vertical;}[type="checkbox"], [type="radio"] {padding: 0;}[type="search"] {-webkit-appearance: textfield;outline-offset: -2px;}::-webkit-inner-spin-button, ::-webkit-outer-spin-button {height: auto;}::-webkit-input-placeholder {color: inherit;opacity: 0.54;}::-webkit-search-decoration {-webkit-appearance: none;}::-webkit-file-upload-button {-webkit-appearance: button;font: inherit;}::-moz-focus-inner {border-style: none;padding: 0;}:-moz-focusring {outline: 1px dotted ButtonText;}details {display: block;}dialog {background-color: white;border: solid;color: black;display: block;height: -moz-fit-content;height: -webkit-fit-content;height: fit-content;left: 0;margin: auto;padding: 1em;position: absolute;right: 0;width: -moz-fit-content;width: -webkit-fit-content;width: fit-content;}dialog:not([open]) {display: none;}summary {display: list-item;}canvas {display: inline-block;}template {display: none;}a, area, button, input, label, select, summary, textarea, [tabindex] {-ms-touch-action: manipulation;touch-action: manipulation;}[hidden] {display: none;}[aria-busy="true"] {cursor: progress;}[aria-controls] {cursor: pointer;}[aria-disabled="true"], [disabled] {cursor: not-allowed;}[aria-hidden="false"][hidden]:not(:focus) {clip: rect(0, 0, 0, 0);display: inherit;position: absolute;}main, header, footer, article, section, aside, details, summary {margin: 0 auto;margin-bottom: 16px;width: 100%;}main {display: block;margin: 0 auto;max-width: 1000px;padding: 0 16px 16px;}footer {border-top: 1px solid rgba(0, 0, 0, 0.12);padding: 16px 0;text-align: left;}footer p {margin-bottom: 0;}hr {border: 0;border-top: 1px solid rgba(0, 0, 0, 0.12);display: block;margin-top: 16px;margin-bottom: 16px;width: 100%;-webkit-box-sizing: content-box;box-sizing: content-box;height: 0;overflow: visible;}img {height: auto;max-width: 100%;vertical-align: baseline;}@media screen and (max-width: 400px) {article, section, aside {clear: both;display: block;max-width: 100%;}img {margin-right: 16px;}}embed, iframe, video {border: 0;}body {color: rgba(0, 0, 0, 0.8);font-family: "Ubuntu", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 16px;line-height: 1.5;}p {margin: 0;margin-bottom: 16px;}h1, h2, h3, h4, h5, h6 {color: inherit;font-family: inherit;line-height: 1.2;font-weight: 500;}h1 {font-size: 40px;margin: 20px 0 16px;}h2 {font-size: 32px;margin: 20px 0 16px;}h3 {color: #75cc00;font-size: 28px;margin: 16px 0 4px;}h4 {color: #75cc00;font-size: 24px;margin: 16px 0 4px;}h5 {color: #75cc00;font-size: 20px;margin: 16px 0 4px;}h6 {color: #75cc00;font-size: 16px;margin: 16px 0 4px;}small {color: rgba(0, 0, 0, 0.54);vertical-align: bottom;}pre {background: #f7f7f9;color: rgba(0, 0, 0, 0.8);display: block;font-family: "Roboto Mono", Menlo, Monaco, Consolas, "Courier New", monospace;font-size: 16px;margin: 16px 0;padding: 16px;white-space: pre-wrap;overflow-wrap: break-word;}code {background: #f7f7f9;color: rgba(0, 0, 0, 0.8);font-family: "Roboto Mono", Menlo, Monaco, Consolas, "Courier New", monospace;font-size: 16px;line-height: inherit;margin: 0;vertical-align: baseline;word-break: break-all;word-wrap: break-word;}a {color: #75cc00;text-decoration: none;background-color: transparent;}a:hover, a:focus {color: #0062cc;font-weight: bolder;text-decoration: underline;}dl {margin-bottom: 16px;}dd {margin-left: 40px;}ul, ol {margin-bottom: 8px;padding-left: 40px;vertical-align: baseline;}blockquote {border-left: 2px solid rgba(0, 0, 0, 0.8);font-family: Georgia, Times, "Times New Roman", serif;font-style: italic;margin: 16px 0;padding-left: 16px;}figcaption {font-family: Georgia, Times, "Times New Roman", serif;}u {text-decoration: underline;}s {text-decoration: line-through;}sup {font-size: 14px;vertical-align: super;}sub {font-size: 14px;vertical-align: sub;}mark {background: #ffeb3b;}input[type="text"], input[type="password"], input[type="email"], input[type="url"], input[type="date"], input[type="month"], input[type="time"], input[type="datetime"], input[type="datetime-local"], input[type="week"], input[type="number"], input[type="search"], input[type="tel"], select, textarea {background: #fff;background-clip: padding-box;border: 1px solid rgba(0, 0, 0, 0.12);border-radius: 4px;color: rgba(0, 0, 0, 0.8);display: block;width: 100%;padding: 8px 16px;line-height: 1.5;-webkit-transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";}input[type="color"] {background: #fff;border: 1px solid rgba(0, 0, 0, 0.12);border-radius: 4px;display: inline-block;vertical-align: middle;}input:not([type]) {-webkit-appearance: none;background: #fff;background-clip: padding-box;border: 1px solid rgba(0, 0, 0, 0.12);border-radius: 4px;color: rgba(0, 0, 0, 0.8);display: block;width: 100%;padding: 8px 16px;line-height: 1.5;-webkit-transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out;transition: border-color .15s ease-in-out, box-shadow .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;text-align: left;}input[type="text"]:focus, input[type="password"]:focus, input[type="email"]:focus, input[type="url"]:focus, input[type="date"]:focus, input[type="month"]:focus, input[type="time"]:focus, input[type="datetime"]:focus, input[type="datetime-local"]:focus, input[type="week"]:focus, input[type="number"]:focus, input[type="search"]:focus, input[type="tel"]:focus, input[type="color"]:focus, select:focus, textarea:focus {background-color: #fff;border-color: #80bdff;outline: 0;-webkit-box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);}input:not([type]):focus {background-color: #fff;border-color: #80bdff;outline: 0;-webkit-box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);}input[type="file"]:focus, input[type="radio"]:focus, input[type="checkbox"]:focus {outline: 1px thin rgba(0, 0, 0, 0.12);}input[type="text"][disabled], input[type="password"][disabled], input[type="email"][disabled], input[type="url"][disabled], input[type="date"][disabled], input[type="month"][disabled], input[type="time"][disabled], input[type="datetime"][disabled], input[type="datetime-local"][disabled], input[type="week"][disabled], input[type="number"][disabled], input[type="search"][disabled], input[type="tel"][disabled], input[type="color"][disabled], select[disabled], textarea[disabled] {background-color: rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.54);cursor: not-allowed;opacity: 1;}input:not([type])[disabled] {background-color: rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.54);cursor: not-allowed;opacity: 1;}input[readonly], select[readonly], textarea[readonly] {border-color: rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.54);}input:focus:invalid, textarea:focus:invalid, select:focus:invalid {border-color: #ea1c0d;color: #f44336;}input[type="file"]:focus:invalid:focus, input[type="radio"]:focus:invalid:focus, input[type="checkbox"]:focus:invalid:focus {outline-color: #f44336;}select {border: 1px solid rgba(0, 0, 0, 0.12);vertical-align: sub;}select:not([size]):not([multiple]) {height: -webkit-calc(2.25rem + 2px);height: calc(2.25rem + 2px);}select[multiple] {height: auto;}label {display: inline-block;line-height: 2;}fieldset {border: 0;margin: 0;padding: 8px 0;}legend {border-bottom: 1px solid rgba(0, 0, 0, 0.12);color: rgba(0, 0, 0, 0.8);display: block;margin-bottom: 8px;padding: 8px 0;width: 100%;}textarea {overflow: auto;resize: vertical;}input[type=checkbox], input[type=radio] {-webkit-box-sizing: border-box;box-sizing: border-box;padding: 0;display: inline;}input[type=submit], input[type=reset], input[type=button], button {background-color: #75cc00;border: #75cc00;border-radius: 4px;color: #fff;padding: 8px 16px;display: inline-block;font-weight: 400;text-align: center;white-space: nowrap;vertical-align: middle;-webkit-user-select: none;-moz-user-select: none;-ms-user-select: none;user-select: none;border: 1px solid transparent;font-size: 1rem;line-height: 1.5;-webkit-transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, box-shadow .15s ease-in-out;transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, box-shadow .15s ease-in-out, -webkit-box-shadow .15s ease-in-out;}input[type=submit]::-moz-focus-inner, input[type=reset]::-moz-focus-inner, input[type=button]::-moz-focus-inner, button::-moz-focus-inner {padding: 0;}input[type=submit]:hover, input[type=reset]:hover, input[type=button]:hover, button:hover {background-color: #0069d9;border-color: #0062cc;color: #fff;}input[type=submit]:not(:disabled):active, input[type=reset]:not(:disabled):active, input[type=button]:not(:disabled):active, button:not(:disabled):active {background-color: #0062cc;border-color: #005cbf;color: #fff;}input[type=submit]:focus, input[type=reset]:focus, input[type=button]:focus, button:focus {outline: 0;-webkit-box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.5);box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.5);}input[type=submit]:disabled, input[type=reset]:disabled, input[type=button]:disabled, button:disabled {opacity: .65;cursor: not-allowed;background-color: #75cc00;border-color: #75cc00;color: #fff;}table {border-top: 1px solid rgba(0, 0, 0, 0.12);margin-bottom: 16px;}caption {padding: 8px 0;}thead th {border: 0;border-bottom: 2px solid rgba(0, 0, 0, 0.12);text-align: left;}tr {margin-bottom: 8px;}th, td {border-bottom: 1px solid rgba(0, 0, 0, 0.12);padding: 16px;white-space: nowrap;vertical-align: inherit;}tfoot tr {text-align: left;}tfoot td {color: rgba(0, 0, 0, 0.54);font-size: 8px;font-style: italic;padding: 16px 4px;}a.skip-main {left:-999px;position:absolute;top:auto;width:1px;height:1px;overflow:hidden;z-index:-999;}a.skip-main:focus, a.skip-main:active {color: #fff;background-color:#000;left: auto;top: auto;width: 30%;height: auto;overflow:auto;margin: 10px 35%;padding:5px;border-radius: 15px;border:4px solid yellow;text-align:center;font-size:1.2em;z-index:999;}@font-face {font-family: 'Raleway';font-style: normal;font-weight: 600;src: url('/s/inc/css/raleway-v18-latin-600.eot');src: local(''), url('/s/inc/css/raleway-v18-latin-600.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/raleway-v18-latin-600.woff2') format('woff2'), url('/s/inc/css/raleway-v18-latin-600.woff') format('woff'), url('/s/inc/css/raleway-v18-latin-600.ttf') format('truetype'), url('/s/inc/css/raleway-v18-latin-600.svg#Raleway') format('svg');}@font-face {font-family: 'Raleway';font-style: italic;font-weight: 400;src: url('/s/inc/css/raleway-v18-latin-italic.eot');src: local(''), url('/s/inc/css/raleway-v18-latin-italic.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/raleway-v18-latin-italic.woff2') format('woff2'), url('/s/inc/css/raleway-v18-latin-italic.woff') format('woff'), url('/s/inc/css/raleway-v18-latin-italic.ttf') format('truetype'), url('/s/inc/css/raleway-v18-latin-italic.svg#Raleway') format('svg');}@font-face {font-family: 'Roboto Mono';font-style: normal;font-weight: 400;src: url('/s/inc/css/roboto-mono-v12-latin-regular.eot');src: local(''), url('/s/inc/css/roboto-mono-v12-latin-regular.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/roboto-mono-v12-latin-regular.woff2') format('woff2'), url('/s/inc/css/roboto-mono-v12-latin-regular.woff') format('woff'), url('/s/inc/css/roboto-mono-v12-latin-regular.ttf') format('truetype'), url('/s/inc/css/roboto-mono-v12-latin-regular.svg#RobotoMono') format('svg');}@font-face {font-family: 'Roboto Mono';font-style: normal;font-weight: 600;src: url('/s/inc/css/roboto-mono-v12-latin-600.eot');src: local(''), url('/s/inc/css/roboto-mono-v12-latin-600.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/roboto-mono-v12-latin-600.woff2') format('woff2'), url('/s/inc/css/roboto-mono-v12-latin-600.woff') format('woff'), url('/s/inc/css/roboto-mono-v12-latin-600.ttf') format('truetype'), url('/s/inc/css/roboto-mono-v12-latin-600.svg#RobotoMono') format('svg');}@font-face {font-family: 'Ubuntu';font-style: normal;font-weight: 400;src: url('/s/inc/css/ubuntu-v15-latin-regular.eot');src: local(''), url('/s/inc/css/ubuntu-v15-latin-regular.eot?#iefix') format('embedded-opentype'), url('/s/inc/css/ubuntu-v15-latin-regular.woff2') format('woff2'), url('/s/inc/css/ubuntu-v15-latin-regular.woff') format('woff'), url('/s/inc/css/ubuntu-v15-latin-regular.ttf') format('truetype'), url('/s/inc/css/ubuntu-v15-latin-regular.svg#Ubuntu') format('svg');}@font-face {font-family:'Raleway2';font-style:normal;font-weight:normal;src:url('/s/inc/css/raleway.eot');src:local('Raleway2'),local('Raleway2'),url('/s/inc/css/raleway.ttf') }.headheader {font-family:"Raleway2"!important }.headheader a {color:#000;text-decoration:none }.headheader a:hover {color:#000;text-decoration:none!important }#toc ul {list-style: none;margin: 0;padding: 0;}#toc h3 {color:black;}</style>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link type="application/opensearchdescription+xml" rel="search" href="/s/inc/opensearch.xml"/>
        <link rel="alternate" type="application/rss+xml" title="RSS Feed for Raymii.org" href="https://raymii.org/s/feed.xml" />         
    </head>
    <body>
        
        <a id="top-of-page"></a>
        <main>
        <a class="skip-main" href="#main">Skip to main content</a>
            <header>
                <h1 class="headheader">
                    <a href="https://raymii.org/s/">Raymii.org 
                        <img src="data:image/png;base64,b'iVBORw0KGgoAAAANSUhEUgAAADIAAAAKCAYAAAD2Fg1xAAABgElEQVQ4jb3VP0iVYRTH8c9waXBokog7OYhTXChuF3GIi4hoiJA4REQIOTgGtoWTg6ODs0SYComIXCJEMhpKtD9guUU0ujRFS0PQ8DzC24v3Pq+3S9/pnMOP8/7Ocx6el/OziRN0JXTD+I2xhK4WdeNteGmbu8IgC3jQQlfCZ0zgINHzJabwoQP+ClHGV1zGJXwRDJ/FDJZi3MBQE10dL2K8gZFOGE3REDZyyjLunKG7KAzZHfMaXjXp+QbXYlzBfrvmSuhBNaHrxQU8zdQW8RhrOe0snuB7zA/jd6p4n9HV8QMfY/4JPzGAt7meFfS18LdXEk7uemIQuJ/Lj6PZQezFWhm3cTWnXcAj3MrU5oWh5WpzGM3UurGNZy28HSa8J7mB3Uy+4u/rl+UdrsT4Jraa6F6jP5M3MP0PHguzL9zzqmC2GRNYjXF2qDzDwgbgHp53wGMhJrEunGQ9oT3CQ+GFasWBsLVvwiv5XygJz/JOAe208POrJHST+CVspBB/AFY9Q3+QJqLxAAAAAElFTkSuQmCC'" alt="Raymii.org Logo">
                    </a>
                </h1>
                <small>
                  Quis custodiet ipsos custodes?<br>
                  <a href="/s/">Home</a> | 
                  <a href="/s/static/About.html">About</a> | 
                  <a href="/s/tags/all.html">All pages</a> | 
                  <a href="/s/software/Sparkling_Network.html">Cluster Status</a> | 
                  <a href="https://raymii.org/s/feed.xml">RSS Feed</a> 
                </small><br/><p>
                <link href="/s/_pagefind/pagefind-ui.css" rel="stylesheet">
                <script src="/s/_pagefind/pagefind-ui.js" type="text/javascript"></script>
                <div id="search" style="min-width:400px;max-width:1080px;"></div>
                <script>
                    window.addEventListener('DOMContentLoaded', (event) => {
                        new PagefindUI({ element: "#search" });
                    });
                </script>
                </p>
            </header>
          
    <main data-pagefind-body><h2 class='headheader' data-pagefind-meta='title' id='main'>Building HA Clusters with Ansible and Openstack</h2>
<p><small>Published: <span data-pagefind-meta='date'>25-07-2014</span> | Author: Remy van Elst | <a href="Building_HA_Clusters_With_Ansible_and_Openstack.txt">Text only version of this article</a>
</small></p>
<br><div class='olderthanayear'><p><strong>&#10071; This post is over 9.824657534246576 years old. It may no longer be up to date. Opinions may have changed.</strong></p></div>
<div id="toc">
<h3>Table of Contents</h3>
<ul>
<li>
<a href="#toc_0">Summary</a>
</li>
<li>
<a href="#toc_1">Openstack Preparations</a>
<ul>
<li>
<a href="#toc_2">Set up a manager host</a>
</li>
<li>
<a href="#toc_3">Creating a network</a>
</li>
<li>
<a href="#toc_4">Network ports for keepalived and floating IP</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">Variable setup</a>
</li>
<li>
<a href="#toc_6">Host Creation</a>
</li>
<li>
<a href="#toc_7">Configuring Loadbalancers</a>
<ul>
<li>
<a href="#toc_8">nginx setup</a>
</li>
<li>
<a href="#toc_9">keepalived setup</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">Database Cluster</a>
<ul>
<li>
<a href="#toc_11">MySQL Master Master</a>
</li>
<li>
<a href="#toc_12">Keepalived for MySQL</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">App servers</a>
<ul>
<li>
<a href="#toc_14">Apache and Deployment</a>
</li>
</ul>
</li>
<li>
<a href="#toc_15">Testing the cluster</a>
</li>
<li>
<a href="#toc_16">What if shit hits the fan?</a>
</li>
</ul>

</div><hr><div id="contents">
<p>This is an extensive guide on building high available clusters with Ansible and
Openstack. We&#39;ll build a Highly available cluster consisting out of two load
balancers, two database servers and two application servers. This is all done
with Ansible, the cluster nodes are all on Openstack. Ansible is a super awesome
orchestration tool and Openstack is a big buzzword filled software suite for
datacenter virtualization.</p>

<h3 id="toc_0">Summary</h3>

<p><img src="https://raymii.org/s/inc/img/openstack-cluster.png" alt=""></p>

<p>This image represents the setup we will create. It will be a simple, data center
redundant high available cluster. The tutorial sets up two nginx frontend load
balancers with keepalived failover, two mysql database servers with master-
master replication and keepalived failover and two application servers with
apache and php plus glusterfs for file syncronization.</p>

<p>We&#39;ll create and provision these VM&#39;s fully with Ansible combined with the magic
of Openstack.</p>

<p>Openstack has the concept of Availability Zones. Your can see this as multiple,
physically independent networks which are logically able to communicate. Most
Cloud providers call them seperate Data centers or Regions. If your Cloud
provider has set it up correctly, the Openstack Cloud, and thus your cluster,
will survive one full datacenter failing.</p>

<p>This tutorial sets up Wordpress as the application, but it can be easily adapted
for any other cluster setup.</p>

<p>I&#39;m using the Openstack Cloud from <a href="https://cloudvps.com">CloudVPS</a>, but any Openstack cloud will
do. CloudVPS is the best one though, in my opinion. The screenshots you&#39;ll see
are not of the default Openstack dashboard but of the CloudVPS Skyline Openstack
Interface.</p>

<p class="ad"> <b>Recently I removed all Google Ads from this site due to their invasive tracking, as well as Google Analytics. Please, if you found this content useful, consider a small donation using any of the options below:</b><br><br> <a href="https://leafnode.nl">I'm developing an open source monitoring app called  Leaf Node Monitoring, for windows, linux & android. Go check it out!</a><br><br> <a href="https://github.com/sponsors/RaymiiOrg/">Consider sponsoring me on Github. It means the world to me if you show your appreciation and you'll help pay the server costs.</a><br><br> <a href="https://www.digitalocean.com/?refcode=7435ae6b8212">You can also sponsor me by getting a Digital Ocean VPS. With this referral link you'll get $100 credit for 60 days. </a><br><br> </p>

<p>The playbook can be found <a href="https://github.com/RaymiiOrg/ansible/tree/master/openstack-example">in this git repository</a>.</p>

<p>I like <a href="https://raymii.org/s/tags/ansible.html">Ansible, I&#39;ve written some more articles about it.</a> I&#39;m also a fan of
<a href="https://raymii.org/s/tags/openstack.html">Openstack, check out my other articles about it.</a>. Last but not least I also
like <a href="https://raymii.org/s/tags/ubuntu.html">Ubuntu</a>.</p>

<h3 id="toc_1">Openstack Preparations</h3>

<p>We need to do a few things before we can start.</p>

<h4 id="toc_2">Set up a manager host</h4>

<p><img src="https://raymii.org/s/inc/img/skyline-hosts.png" alt="network"></p>

<p>We will create all the nodes in a private network, unreachable from the
internet. To access all hosts we need a manager vm where we can run ansible and
debug if needed.</p>

<p>Create a VM in Openstack, I&#39;ll create an Ubuntu 14.04 VM and install Ansible on
it. Give it an internal IP in that network and associate a floating IP with that
to make sure you can SSH in.</p>

<p>Install the packages required for building Ansible:</p>

<pre><code>apt-get install python-pip python-dev
</code></pre>

<p>Install the latest Ansible:</p>

<pre><code>pip install ansible
</code></pre>

<p>Install the openstack nova client:</p>

<pre><code>pip install python-novaclient
</code></pre>

<p>Copy the playbook over and edit the <code>vars/main.yml</code> file with your two port IP&#39;s
and your Openstack credentials and your passwords.</p>

<p>Create a <code>ansible_hosts</code> file with the following contents:</p>

<pre><code>[localhost]
127.0.0.1
</code></pre>

<p>The combination of Openstack and Ansible will create all the hosts and make them
dynamically available to us for use within the playbook.</p>

<h4 id="toc_3">Creating a network</h4>

<p>Use the GUI or CLI to create a network with routing/NAT and DHCP. Note down the
network ID and the subnet ID. The network ID must be changed in the Playbook
Variables.</p>

<p><img src="https://raymii.org/s/inc/img/skyline-graph.png" alt="network"></p>

<h4 id="toc_4">Network ports for keepalived and floating IP</h4>

<p>Create two floating IP&#39;s. Attach one to the Ansible Manager host.</p>

<p><img src="https://raymii.org/s/inc/img/skyline-floating-ips.png" alt="keepalived"></p>

<p>Create two ports in your private network for the two keepalived configurations.
And associate the floating IP with one of these ports.</p>

<p>The first command will create a port in the private network and will return an
port id and IP address which will be used for Keepalived. The VM&#39;s in this
network can bind and use these addresses. The second command will associate the
port we created previously with the floating ip address so that we can bind and
fail over the load balancers with one external (and internal) IP. The commands
look like this:</p>

<pre><code>neutron port-create NETWORK-UUID --tenant-id YOUR_TENANT_ID
neutron floatingip-associate FLOATING_IP_UUID PORT_UUID
</code></pre>

<p>The port-create should be done two times since we&#39;ll use keepalived on the
database servers and on the load balancers.</p>

<p>The two ports will be listed as &quot;DOWN&quot;, even when they are not.</p>

<p><img src="https://raymii.org/s/inc/img/skyline-network.png" alt="network"></p>

<p>I&#39;ll cover all the different parts of the playbook before we run it. Let&#39;s start
with the Host Creation.</p>

<h3 id="toc_5">Variable setup</h3>

<p>Make sure you edit the <code>vars/main.yml</code> file. Change your authentication data,
the HA IP addresses to the ones you received from your Openstack provider. Here
are the most important things to change:</p>

<pre><code>---
auth_url: https://identity.stack.cloudvps.com/v2.0  
image_id: 
private_net: 

keypair_name: SSH_Key
website_name: &quot;example.org&quot;
mysql_user: &quot;app&quot;
mysql_password: &quot;FINE2yfUIt&quot;

keepalived_sql_vip: &quot;10.107.244.210&quot;
keepalived_sql_router_id: &quot;60&quot;
keepalived_sql_passwd: &quot;cc2BgJiGAbAoSlks&quot;

keepalived_lbs_vip: &quot;10.107.244.200&quot;
keepalived_lbs_router_id: &quot;50&quot;
keepalived_lbs_passwd: &quot;m9RwFk3Mx&quot;
</code></pre>

<h3 id="toc_6">Host Creation</h3>

<p>We&#39;ll use the ansible nova_compute module to create all the required hosts. You
should have adapted the variables file with the parameters for your Openstack
Cloud, in this example we will create 6 hosts:</p>

<ul>
<li>2 load balancers running nginx and keepalived</li>
<li>2 database servers running mysql in master-master</li>
<li>2 app servers running apache and php, plus glusterfs for the file sync</li>
</ul>

<p>All the example servers run Ubuntu 14.04.</p>

<p>One of each these servers will be in Availability Zone NL1 and one in zone NL2.
If your Openstack Provider has Availability Zones and you use them correctly
then your cluster will be protected against one whole data center (Availability
Zone) loss.</p>

<p>I&#39;ve added the Availability Zone selection code to Ansible 1.7, <a href="https://github.com/ansible/ansible/pull/8182">see this pull
request</a>. Make sure you run at least Ansible 1.7, otherwise it will complain
about not knowing the parameter. You can also just comment out the availability
zone parameter, apply the patch yourself or run the &quot;devel&quot; branch of Ansible
(when it is merged).</p>

<p>The play consists out of 6 times the following, one for each host:</p>

<pre><code># tasks/create-instances.yml
- nova_compute:
    auth_url: &quot;{{ auth_url }}&quot;
    login_username: &quot;{{ login_username }}&quot;
    login_password: &quot;{{ login_password }}&quot;
    login_tenant_name: &quot;{{ login_tenant_name }}&quot;
    security_groups: &quot;built-in-allow-all&quot;
    state: present
    availability_zone: &quot;NL2&quot;
    name: ansible-cluster-lb1
    image_id: &quot;{{ image_id }}&quot;
    key_name: &quot;{{ keypair_name }}&quot;
    wait_for: 500
    nics:
      - net-id: &quot;{{ private_net }}&quot;
    flavor_id: &quot;{{ flavor_id }}&quot;
    meta:
      hostname: ansible-cluster-lb1
      group: ansible
  register: openstacklb1

- add_host: 
    name: &quot;{{ openstacklb1.private_ip }}&quot; 
    groupname: lbs 
    keepalived_lbs_prio: 150
</code></pre>

<p>We give all the nodes only Internal IP addresses. Since we have a network with
NATing and a router they will all be able to access the internet. All
communication will go via the earlier created manager node, and all web traffic
will go via the virtual/Floating IP.</p>

<p>Make sure you have ssh key forwarding set up. Make sure you have uploaded your
SSH key to Openstack. Check all variables and match them with your Openstack
Cloud. (Image ID, flavor ID etc.)</p>

<p>After the instance has been created we add the host to a new hostgroup,
according to the role it will be in <code>app</code>, <code>lbs</code> or <code>dbs</code>. We also, per host
when applicable, give the keepalived priority and the mysql server ID as Ansible
Variables. These will be used later on in the playbook run.</p>

<p>All hosts should be created by Ansible. Openstack will give them an IP and
Ansible registers that IP for use in a later play.</p>

<h3 id="toc_7">Configuring Loadbalancers</h3>

<p><img src="https://raymii.org/s/inc/img/nginx-alt.png" alt=""></p>

<p>Our load balancer configuration consists out of two nginx servers with a reverse
proxy configuration to both app servers. They are connected to each other via
keepalived and this, if one fails, the other will take over. Because the
keepalived virtual IP is also associated to the floating IP, it wil
automatically work for that as well.</p>

<p>This play is configured to gather facts on both the <code>lbs</code> group and the <code>app</code>
group, but only runs tasks on the <code>lbs</code> group.</p>

<p>We need to do this because we need facts from the <code>app</code> group to configure
nginx, but Ansible only gathers facts from hosts in the play.</p>

<p>We therefore cannot run the playbook just on the load balancer nodes. We also
don&#39;t want to do any actions in on the app nodes, just the facts.</p>

<p>You can define multipe groups where a playbook should run with the <code>:</code> as you
can see below, we run on <code>lbs:app</code>. The <code>when</code> statement makes sure the actual
playbook only runs on the nodes in the <code>lbs</code> group.</p>

<pre><code># tasks/main.yml
- name: Configure LoadBalancers
  hosts: lbs:app
  vars_files:
    - &quot;vars/main.yml&quot;
  user: root
  tasks:
    - include: tasks/configure-lbs.yml
      when: &#39;&quot;{{ inventory_hostname }}&quot; in &quot;{{ groups.lbs }}&quot;&#39;
    - include: tasks/keepalived.yml
      when: &#39;&quot;{{ inventory_hostname }}&quot; in &quot;{{ groups.lbs }}&quot;&#39;
</code></pre>

<h4 id="toc_8">nginx setup</h4>

<p>We start by adding the nginx stable PPA so that we have the latest version of
nginx available:</p>

<pre><code># tasks/configure-lbs.yml
---
- apt_repository: 
    repo: &#39;ppa:nginx/stable&#39;
    state: present
    update_cache: yes
</code></pre>

<p>We install nginx, vim, git and ntp. The first is the load balancer itself, the
next two are tools I frequently use and the last is important, out-of-sync time
can cause weird cluster issues.</p>

<pre><code>- apt: 
    name: &quot;{{ item }}&quot; 
    state: latest 
    update_cache: yes
  with_items:
    - nginx
    - vim
    - git
    - ntp
</code></pre>

<p>The folder for the nginx cache is created:</p>

<pre><code>- file:
    dest: /var/cache/nginx
    state: directory
    owner: www-data
</code></pre>

<p>We then place the loadbalancer config:</p>

<pre><code>- template:
    src: nginx-lb.conf.j2
    dest: /etc/nginx/sites-available/lbs.conf
  register: confresult
  notify:
  - restart keepalived
</code></pre>

<p>The template file looks like this:</p>

<pre><code># templates/nginx.conf.j2
upstream backend  {
{% for host in groups[&#39;app&#39;] %}
    server {{ hostvars[host][&#39;ansible_eth0&#39;][&#39;ipv4&#39;][&#39;address&#39;] }}:80 max_fails=5  fail_timeout=5s;
{% endfor %}
}

proxy_cache_path /var/cache/nginx levels=1:2  keys_zone=CACHE:10m inactive=24h  max_size=1g;

server {
    listen          80  default_server;
    server_name     {{ website_name }};

    access_log  /var/log/nginx/{{ website_name }}.access.log;
    error_log   /var/log/nginx/{{ website_name }}.error.log;
    root        /usr/share/nginx/html;

    location / {
        proxy_pass              http://backend;
        proxy_next_upstream     error timeout invalid_header http_500 http_502 http_503 http_504;
        proxy_redirect          off;
        proxy_cache             CACHE;
        proxy_cache_valid       200  1d;
        proxy_cache_use_stale   error timeout invalid_header updating  http_500 http_502 http_503 http_504;
        proxy_set_header        Host            $host;
        proxy_set_header        X-Real-IP       $remote_addr;
        proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
   }
}
</code></pre>

<p>As you can see it uses Jinja2 loops to add every app-server to the backend using
this piece of logic:</p>

<pre><code>{% for host in groups[&#39;app&#39;] %}
    server {{ hostvars[host][&#39;ansible_eth0&#39;][&#39;ipv4&#39;][&#39;address&#39;] }}:80 max_fails=5  fail_timeout=5s;
{% endfor %}
</code></pre>

<p>If we want to spawn more app servers we can do that and they will be
automatically configured here on the next ansible run.</p>

<p>This configuration will remove a backend server if it gives more than 5 non 2XX
or 3XX HTTP status coedes or if it does not respond in 5 seconds. If one app
server is down, the load balancer will not send more visitors to it.</p>

<pre><code>proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
</code></pre>

<p>This configuration also has caching enabled:</p>

<pre><code>proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=CACHE:10m  max_size=1g;
proxy_cache             CACHE;
proxy_cache_valid       200  1h;
proxy_cache_use_stale   error timeout invalid_header updating  http_500 http_502 http_503 http_504;
</code></pre>

<p>This will cache all data which was sent with a 200 response for 1 hour. All data
that is cached but not accessed is removed from the cache after 10 minutes.</p>

<p>Make sure you send a <code>Cache-Control</code> header: <code>Cache-Control: max-age=900, must-
revalidate</code> otherwise you might run into caching problems. Read the <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html">rfc
here</a>. If you are unsure, turn it off.</p>

<p>We link the file to the <code>sites-enabled</code> folder, as a good practice:</p>

<pre><code>- file:
    src: /etc/nginx/sites-available/lbs.conf
    dest: /etc/nginx/sites-enabled/lbs.conf
    state: link
  notify:
  - restart keepalived
</code></pre>

<p>This means that when your fellow sysadmin removes the file in sites-enabled/ it
will not be gone forever, but just disabled.</p>

<p>We make sure the default site is not running:</p>

<pre><code>- file:
    dest: /etc/nginx/sites-enabled/default
    state: absent
  notify:
  - restart keepalived
</code></pre>

<p>nginx is restarted when needed by the notify handler.</p>

<h4 id="toc_9">keepalived setup</h4>

<p><img src="https://raymii.org/s/inc/img/keepalived-portal.png" alt=""></p>

<p>keepalived will provide basic IP failover. If one node fails the other will take
over. Ansible will make sure the confituration everywhere is the same, and we
have created a port and floating IP which will we connect visitors to so that in
the case of a failover everything keeps working.</p>

<p>Keepalived is very simple to setup, <a href="https://raymii.org/s/tutorials/Keepalived-Simple-IP-failover-on-Ubuntu.html">I&#39;ve written an article about the setup on
Ubuntu 14.04 which you should read</a>.</p>

<p>First we install keepalived on both nodes:</p>

<pre><code># tasks/keepalived.yml
---
- apt: 
    name: &quot;{{ item }}&quot; 
    state: latest 
    update_cache: yes
  with_items:
    - keepalived
</code></pre>

<p>We make sure linux can bind to an address not configured on an interface (the
VIP, for example):</p>

<pre><code>- sysctl: 
    name: net.ipv4.ip_nonlocal_bind 
    value: &quot;1&quot; 
    state: present
    reload: yes
</code></pre>

<p>If we don&#39;t do that, keepalived will not work.</p>

<p>We place the config file. The playbook places a different config file for the
load balancers and for the database servers, this because the Virtual IP, router
ID and password are different for both configurations.</p>

<p>These values are used by Keepalived to determine the VIP, nodes and other
configuration. We can run multiple keepalived instances in the same subnet, but
we need to make sure they have a different ID and password.</p>

<pre><code>- template:
    src: keepalived.lbs.conf.j2
    dest: /etc/keepalived/keepalived.conf
  when: inventory_hostname in groups[&#39;lbs&#39;]
  notify:
  - restart keepalived

- template:
    src: keepalived.sql.conf.j2
    dest: /etc/keepalived/keepalived.conf
  when: inventory_hostname in groups[&#39;dbs&#39;]
  notify:
  - restart keepalived
</code></pre>

<p>You can check out the <a href="https://raymii.org/s/inc/img/database-dilbert.png">git repository</a> to see the keepalived layout. Since
it is fairly standard, I won&#39;t show it here.</p>

<p>Do note that it only protects against full host failure, not just failure of the
webserver. That is some homework for you to figure out.</p>

<p>If the config has changed, we restart the keepalived service via the notify
handler.</p>

<p>We now have configured the load balancers. Let&#39;s move on to the Database
Servers.</p>

<h3 id="toc_10">Database Cluster</h3>

<p><img src="https://raymii.org/s/inc/img/database-dilbert.png" alt=""></p>

<p>Our database cluster will consist out of two MySQL servers running in Master
Master mode.</p>

<p>This play runs on the two database servers Ansible created earlier:</p>

<pre><code># tasks/main.yml
- name: Configure Databases
  hosts: dbs
  vars_files:
    - &quot;vars/main.yml&quot;
  user: root
  tasks:
    - include: tasks/configure-dbs.yml
    - include: tasks/keepalived.yml
  handlers:
    - include: handlers/main.yml
</code></pre>

<h4 id="toc_11">MySQL Master Master</h4>

<p>We start with installing MySQL and the python module Ansible needs for MySQL:</p>

<pre><code># tasks/configure-dbs.yml
---
- apt: 
    name=&quot;{{ item }}&quot; 
    state=latest 
    update_cache=yes
  with_items:
    - mysql-server
    - python-mysqldb
    - vim
    - git
    - ntp
</code></pre>

<p>We place our MySQL config file:</p>

<pre><code>- template:
    src: my.cnf.j2
    dest: /etc/mysql/my.cnf
  notify:
    - restart mysql
</code></pre>

<p>This is the config file:</p>

<pre><code># templates/my.cnf.j2
[client]
port        = 3306
socket      = /var/run/mysqld/mysqld.sock

[mysqld_safe]
socket      = /var/run/mysqld/mysqld.sock
nice        = 0

[mysqld]
user        = mysql
pid-file    = /var/run/mysqld/mysqld.pid
socket      = /var/run/mysqld/mysqld.sock
port        = 3306
basedir     = /usr
datadir     = /var/lib/mysql
tmpdir      = /tmp

lc-messages-dir             = /usr/share/mysql
skip-external-locking
bind-address                = 0.0.0.0
key_buffer                  = 16M
max_allowed_packet          = 16M
thread_stack                = 192K
thread_cache_size           = 8
myisam-recover              = BACKUP
query_cache_limit           = 1M
query_cache_size            = 16M
log_error                   = /var/log/mysql/error.log
log_bin                     = mysql-bin
binlog_do_db                = {{ mysql_user }}
expire_logs_days            = 10
max_binlog_size             = 100M
auto_increment_offset       = 1
auto_increment_increment    = 2
server_id                   = {{ sql_server_id }}

[mysqldump]
quick
quote-names
max_allowed_packet      = 16M

[mysql]
[isamchk]
key_buffer              = 16M

!includedir /etc/mysql/conf.d/
</code></pre>

<p>It is a default Ubuntu config file. The master master configuration consists out
of the following:</p>

<pre><code>log_bin                     = mysql-bin
binlog_do_db                = {{ mysql_user }} # this database will be replicated
expire_logs_days            = 10
max_binlog_size             = 100M
auto_increment_offset       = 1
auto_increment_increment    = 2 # avoid primary key conflicts
server_id                   = {{ sql_server_id }} # variable set during ansible host creation
</code></pre>

<p>We also copy the debian mysql config file to <code>/root/.my.cnf</code>. Ansible needs this
to connect to the database later on. Ansible has no way to copy a file on a host
to another file on that host, so we need to fetch it first and copy it then to
the new location. I use the hostname in the <code>dest</code> otherwise one server would
receive the wrong credentials file.</p>

<pre><code>- fetch:
    src: /etc/mysql/debian.cnf 
    flat: yes
    dest: &quot;/tmp/my.cnf.{{ ansible_hostname }}&quot;

- copy:
    src: &quot;/tmp/my.cnf.{{ ansible_hostname }}&quot;
    dest: /root/.my.cnf
</code></pre>

<p>We create the MySQL database and user for that database on both nodes:</p>

<pre><code>- mysql_user: 
    name: &quot;{{ mysql_user }}&quot; 
    password: &quot;{{ mysql_password }}&quot;
    host: &quot;%&quot;
    priv: &#39;{{ mysql_user }}.*:ALL&#39;
    state: present 

- mysql_db: 
    name: &quot;{{ mysql_user }}&quot;
    state: present
</code></pre>

<p>I use the database username as the database name. This can be changed if needed.</p>

<p>The replication user is made on both nodes, with the correct permissions:</p>

<pre><code>- mysql_user: 
    name: &quot;replicator&quot; 
    host: &quot;%&quot; 
    password: &quot;{{ mysql_password }}&quot;
    priv: &quot;*.*:REPLICATION SLAVE&quot;
    state: present
  notify:
    - restart mysql
</code></pre>

<p>The following part has some Ansible Magic. The mysql_replication module only
works once, it does not seem to be that much idempotent. If you run it again, it
will fail and complain that the slave must be stopped first before it change its
master. That is the expected behaviour, so we only want to set up the
replication if that hasn&#39;t been done yet.</p>

<p>Therefore we first check for the existence of a file. If that file exists, the
replication setup is skipped. If that file does not exist, we set up replication
and then create that file. The file creation is done after the replication setup
is done, so if the first one fails the latter one fails as well.</p>

<pre><code>- stat: path=/etc/mysql/ansible.repl
  register: check_sql_path
</code></pre>

<p>The existense of the file or folder can then later on be checked like so:</p>

<pre><code>when: check_sql_path.stat.exists == false # or true.
</code></pre>

<p>We also use a double <code>when</code> conditional. We need to set up the servers with each
other&#39;s IP as the master. Therefore we need to run the setup for host A with the
master IP of host B, and vice versa.</p>

<p>However, we also need to check for that file. Luckally Ansible supports <code>and</code>
and <code>or</code> in their <code>when</code> conditionals. As you can see we extract the hostname of
the other database node (<code>master_host: &quot;{{ groups.dbs[1] }}&quot;</code>), which in our
case will be the IP address Openstack returns.</p>

<pre><code>- mysql_replication: 
    mode: changemaster 
    master_host: &quot;{{ groups.dbs[1] }}&quot; 
    master_user: replicator 
    master_password: &quot;{{ mysql_password }}&quot;
  when: check_sql_path.stat.exists == false and &#39;{{ inventory_hostname }}&#39; == &#39;{{ groups.dbs[0] }}&#39;
  notify:
    - restart mysql

- mysql_replication: 
    mode: changemaster 
    master_host: &quot;{{ groups.dbs[0] }}&quot; 
    master_user: replicator 
    master_password: &quot;{{ mysql_password }}&quot;
  when: check_sql_path.stat.exists == false and &#39;{{ inventory_hostname }}&#39; == &#39;{{ groups.dbs[1] }}&#39;
  notify:
    - restart mysql
</code></pre>

<p>After this has succeeded the files are created:</p>

<pre><code>- command: touch /etc/mysql/repl.ansible
  when: check_sql_path.stat.exists == false and &#39;{{ inventory_hostname }}&#39; == &#39;{{ groups.dbs[1] }}&#39;

- command: touch /etc/mysql/repl.ansible
  when: check_sql_path.stat.exists == false and &#39;{{ inventory_hostname }}&#39; == &#39;{{ groups.dbs[1] }}&#39;
</code></pre>

<p>We&#39;ve now got two servers running in master master replication mode. You can
test this later on when the wordpress is set up by logging in to db server 1 and
check the tables in the database. Then check the same on the second db server.</p>

<h4 id="toc_12">Keepalived for MySQL</h4>

<p>The keepalive playbook is the same as the one we used in the load balancer
setup. However, we check in which group the hostname is and based on that place
a different config file:</p>

<pre><code># tasks/keepalived.yml
- template:
    src: keepalived.lbs.conf.j2
    dest: /etc/keepalived/keepalived.conf
  when: inventory_hostname in groups[&#39;lbs&#39;]
  notify:
    - restart keepalived

- template:
    src: keepalived.sql.conf.j2
    dest: /etc/keepalived/keepalived.conf
  when: inventory_hostname in groups[&#39;dbs&#39;]
  notify:
    - restart keepalived
</code></pre>

<p>During the creation of the host we also set a different priority for the
database hosts, so that is handled as well.</p>

<p>The keepalived setup is again very simple. Move on the the last part, the actual
application servers.</p>

<h3 id="toc_13">App servers</h3>

<p><img src="https://raymii.org/s/inc/img/stupid-questions.png" alt=""></p>

<p>The app servers are two Apache servers with PHP and GlusterFS for the file sync.</p>

<p>Glusterfs is set up first so that when we deploy the app the changes wil
automagically arrive on the second node:</p>

<pre><code># tasks/main.yml
- name: Configure App Server
  hosts: app
  vars_files:
    - &quot;vars/main.yml&quot;
  user: root
  tasks:
    - include: tasks/configure-gluster-app.yml
    - include: tasks/configure-app.yml
  handlers:
    - include: handlers/main.yml
</code></pre>

<p>Because of <a href="https://bugs.launchpad.net/ubuntu/+source/glusterfs/+bug/1268064">a bug</a> in the Ubuntu Glusterfs packages we need to enable the
official PPA and use newer glusterfs packages. Otherwise your OS will fail to
boot, miserably.</p>

<p>Little rant here, this bug was in some form in 12.04, you would think that 3
years and loads of bug reports would have fixed in 14.04, but that is still not
the case. It seems that if you want a stable distro you still need Red Hat...
End rant.</p>

<pre><code># tasks/configure-gluster-app.yml
---
# boot bug in standard 14.04 packages: https://bugs.launchpad.net/ubuntu/+source/glusterfs/+bug/1268064
- apt_repository: 
    repo: &#39;ppa:semiosis/ubuntu-glusterfs-3.4&#39;
    state: present
    update_cache: yes
</code></pre>

<p>Our application servers are both glusterfs servers and clients of one another.
You can set up a seperate glusterfs cluster, however that would be out of scope
for this tutorial. If you expect lots of IO, it is a recommendation to do.</p>

<p>We install both the server and te client:</p>

<pre><code>- apt:
    name: &quot;{{ item }}&quot; 
    state: installed 
    update_cache: yes
  with_items:
    - glusterfs-server
    - glusterfs-client
    - ntp
</code></pre>

<p>Glusterfs has the concepts of volumes and bricks. A volume is served by the
servers and mounted on the clients. A volume can consist out of multiple bricks,
either providing striping or replication. In our case we go for replication,
that means that both application servers have all the data.</p>

<p>I recommend you rad more on glusterfs, it is a really awesome product and I know
of a few</p>

<p>We make sure the actual brick folder exists:</p>

<pre><code>- file: 
    path: &quot;{{ gluster_brick_dir }}&quot; 
    state: directory
</code></pre>

<p>There is no glusterfs module for Ansible, so here again we need to work around
to prevent some idempotency issues. We make sure all the nodes know about each
other:</p>

<pre><code>- shell: &quot;gluster peer probe {{ item }}&quot;
  with_items:
    - &quot;{{ groups.app }}&quot;
</code></pre>

<p>Then we get a list of all glusterfs nodes for use later on. Ansible gives me
back a nice python-style list in the form of <code>u[10.1.1.2], u[10.1.1.3]</code> which is
of no use in the later gluster commands, therefore we apply some sed
replacements to filter and keep only the IP addresses:</p>

<pre><code>- shell: &#39;echo {{ groups.app }} | sed -e &quot;s/\]//g&quot; -e &quot;s/, u/, /g&quot; -e &quot;s/\[u//g&quot; -e &quot;s%,%:{{ gluster_brick_dir }} %g; s%$%:{{ gluster_brick_dir }}%&quot;&#39;
  register: gluster_bricks
  connection: local
</code></pre>

<p>We check if the volume already exists by issuing a <code>volume info</code> command. If
that fails, the <code>||</code> will make sure the other action happens, that is create the
volume. It is a replicated volume, with the data on at least two nodes. The
output of the sed command from earlier is used to specify all the nodes. We also
only do this on the first glusterfs server, it will otherwise fail on both or on
the other.</p>

<pre><code>- shell: &#39;gluster volume info {{ gluster_volume }} || 
          gluster volume create {{ gluster_volume }} transport tcp replica 2
          {{ gluster_bricks.stdout }} force&#39;
  when: &#39;&quot;{{ inventory_hostname }}&quot; == &quot;{{ groups.app[0] }}&quot;&#39;
</code></pre>

<p>We wait a while to make sure the volume is actually being created. Without this
delay the next steps would fail for me.</p>

<pre><code>- wait_for: 
    delay: 10
    timeout: 10
</code></pre>

<p>We check if the volume is already started, if not, we start it, on both nodes:</p>

<pre><code>- shell: &#39;gluster volume info {{ gluster_volume }} | grep &quot;Status: Started&quot; || 
          gluster volume start {{ gluster_volume }}&#39;
</code></pre>

<p>We make sure the <code>/var/www/html</code> folder exists, since this is the folder we will
be mounting the glusterfs on:</p>

<pre><code>- file: 
    path: &quot;/var/www/html&quot; 
    state: directory
</code></pre>

<p>We mount the volume. The ansible module will also place this in <code>/etc/fstab</code> so
we don&#39;t need to edit that as well:</p>

<ul>
<li>mount: name: /var/www/html fstype: glusterfs src: &quot;{{ groups.app[0] }}:{{ gluster_volume }}&quot; state: mounted</li>
</ul>

<p>If we now create a file on one node, for example <code>/var/www/html/test</code>, this file
should appear on the other node as well.</p>

<p>Gluster cluster information can be found on either hosts with the <code>gluster
volume info</code> or <code>gluster peer info</code> commands.</p>

<p>We can now deploy the actual application.</p>

<h4 id="toc_14">Apache and Deployment</h4>

<p>We start by installing the Apache webserver, php (mod_php) and a few tools and
modules:</p>

<pre><code># tasks/configure-app.yml
---
- apt:
    name=&quot;{{ item }}&quot; 
    state=latest 
    update_cache=yes
  with_items:
    - php5-mysql
    - python-pip
    - php5 
    - libapache2-mod-php5 
    - php5-mcrypt
    - vim
    - git
    - ntp
</code></pre>

<p>We are going to deploy wordpress right out of the git repository:</p>

<pre><code>- git: 
    repo: https://github.com/WordPress/WordPress.git 
    dest: /var/www/html/site
    force: yes
    update: no
  when: &#39;&quot;{{ inventory_hostname }}&quot; == &quot;{{ groups.app[0] }}&quot;&#39;
</code></pre>

<p>If you want to check out a specific version then give a branch name:</p>

<pre><code>version: 3.9-branch
</code></pre>

<p>We only do this on the first node since Gluster will take care of the sync to
the other node.</p>

<p>We remove the default <code>index.html</code> file and replace it with our simple
<code>index.php</code> that redirects to <code>/site</code> where wordpress is located:</p>

<pre><code>- file:
    dest: /var/www/html/index.html
    state: absent

- copy:
    src: index.php
    dest: /var/www/html/index.php
  when: &#39;&quot;{{ inventory_hostname }}&quot; == &quot;{{ groups.app[0] }}&quot;&#39; 
</code></pre>

<p>We deploy the <code>wp-config.php</code> file with our database settings pointing to the
keepalived database IP:</p>

<pre><code>- template:
    src: wp-config.php.j2
    dest: /var/www/html/site/wp-config.php
  when: &#39;&quot;{{ inventory_hostname }}&quot; == &quot;{{ groups.app[0] }}&quot;&#39;
</code></pre>

<p><img src="https://raymii.org/s/inc/img/wordpress-install.png" alt=""></p>

<p>You can now navigate to the floating IP address and start the wordpress
installation.</p>

<h3 id="toc_15">Testing the cluster</h3>

<p>When it is all up and running and you have successfully deployed your
application you of course want to know if the cluster setup works or not. How to
do that best other that just shutting down all nodes in one availability zone?
Or just the database servers or the load balancers? Try it out, start them up
again, see that it all still works.</p>

<p>Congratulations, you&#39;ve now got an awesome cluster setup running, all deployed
via Ansible and Openstack!</p>

<p><img src="https://raymii.org/s/inc/img/youre-awesome.png" alt=""></p>

<p><a href="https://github.com/RaymiiOrg/ansible/tree/master/openstack-example">Repository</a>.</p>

<h3 id="toc_16">What if shit hits the fan?</h3>

<p><img src="https://raymii.org/s/inc/img/shtf.png" alt=""></p>

<p>You now have your awesome cluster setup and working. As you know, this cluster
setup protects against full host failure per layer, thus providing protection
against one full availability zone being offline.</p>

<p>In the case that happens, your site will still be working. However, what do do
when the Availability Zone issue is fixed?</p>

<p>Make sure your servers in the affected zone are offline.</p>

<p>Start with the load balancer, turn it on and check the logs to see if it picks
up keepalived. If it had the highest priority it should pick up the VIP again.
If it had a lower priority, try stopping keepalived on the master and see if it
fails over. If so, make sure the nginx config still works and you are good to
go.</p>

<p>Start up the application server. Make sure it starts correctly. Wait a while for
GlusterFS to sync the files over. Test the sync by creating a few files on one
node and check the other node if they are there as well. Then stop the webserver
on the node that kept working during the crash and test the website. If that all
works you are good to go.</p>

<p>The database is a trickier part. If it has diverged too much, as in, it has been
offline to long you will need to fix the mysql replication. Stop the slave on
both nodes. Also stop the master. Create a dump of the database srever that kept
working and import it on the other db server. Check the binlog positions and
start up replication manually again with the correct file and position.</p>

<p><a href="https://raymii.org/s/inc/img/database-dilbert.png">17</a>: </p>
Tags: <a href="../tags/ansible.html">ansible</a>
, <a href="../tags/articles.html">articles</a>
, <a href="../tags/cloud.html">cloud</a>
, <a href="../tags/compute.html">compute</a>
, <a href="../tags/dashboard.html">dashboard</a>
, <a href="../tags/glusterfs.html">glusterfs</a>
, <a href="../tags/ha.html">ha</a>
, <a href="../tags/keepalived.html">keepalived</a>
, <a href="../tags/mysql.html">mysql</a>
, <a href="../tags/openstack.html">openstack</a>
, <a href="../tags/python.html">python</a>
, <a href="../tags/ubuntu.html">ubuntu</a>
, <a href="../tags/wordpress.html">wordpress</a>
</div></main>
<br/>
<footer>
<br>
                <p><small>
                <a href="/s/">Home</a> | 
                <a href="/s/static/About.html">About</a> | 
                <a href="/s/tags/all.html">All pages</a> | 
                <a href="/s/software/Sparkling_Network.html">Cluster Status</a> | 
                Generated by <a href="/s/software/ingsoc.html">ingsoc</a>.</small>
                </p>
    
    </footer>
    <script data-goatcounter="https://raymii.goatcounter.com/count"
            async src="//gc.zgo.at/count.js"></script>

    <script defer src="/s/inc/js/instant.5.2.0.js"  type="module" ></script>

     
    </main>
    </body>
    </html>
    